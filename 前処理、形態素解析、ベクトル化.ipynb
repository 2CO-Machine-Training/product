{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"前処理、形態素解析、ベクトル化.ipynb","provenance":[],"collapsed_sections":["37aVJcMQ20LI","5ezGBGVPyl7i","L1Fv87VEA6z_","FK6Eozu1OxGo","09Xl8ciUX4gI","FyrCW8QySJ0C","EcFlsEiNYnRM","7fBNaAVyb2iD","dFX_5ubQBT-T","cl1MghTaoS5u"],"toc_visible":true,"authorship_tag":"ABX9TyMd/4PeGuG2ewRE3Ugll1ZV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yNJulcrxM2Kd"},"source":["処理の流れ\n","\n","生文 → 形態素解析 → 数値化（ベクトル化）\n","\n","独立請求項の生文が入力されている列の1行目に「独立請求項」という名前がつけられているexcelデータを \n","\n","'sample_data.xlsx' として 'data/'フォルダに保存しておけばサンプルプログラムが動作します。"]},{"cell_type":"code","metadata":{"id":"OBJpPJMUTSW2","executionInfo":{"status":"ok","timestamp":1602196939363,"user_tz":-540,"elapsed":17484,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"7dfcbd07-91db-4b13-e67a-2128ba3ab8b9","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import os\n","from google.colab import drive\n","\n","# googleドライブのマウント \n","drive.mount('/content/drive/')\n","\n","# 作業ファイルをマイドライブに変更\n","os.chdir('/content/drive/My Drive/') "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jyE1Y_o71N9Z","executionInfo":{"status":"ok","timestamp":1601718820886,"user_tz":-540,"elapsed":1092,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"66497a10-b726-4191-d76c-a5ed88a9bbee","colab":{"base_uri":"https://localhost:8080/","height":785}},"source":["# pandasモジュールを用いたサンプルデータの読み込み\n","\n","import pandas as pd\n","\n","data = pd.read_excel('data/sample_data.xlsx')\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>公報番号(全文リンク)</th>\n","      <th>発明等の名称</th>\n","      <th>出願人・権利者(最新)</th>\n","      <th>ＩＰＣ(最新)</th>\n","      <th>ＦＩ(最新)</th>\n","      <th>Ｆターム(最新)</th>\n","      <th>独立請求項</th>\n","      <th>筆頭FI</th>\n","      <th>出願日</th>\n","      <th>インキ</th>\n","      <th>機構</th>\n","      <th>受容シート</th>\n","      <th>用途</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>特開2019-218636</td>\n","      <td>染色物の製造方法、染色調整インクジェットインク、及び染色調整インクジェットインクセット</td>\n","      <td>株式会社ミマキエンジニアリング</td>\n","      <td>D06P5/00;C09D11/54;C09D11/32;C09D11/40;D06P5/3...</td>\n","      <td>D06P  5/00    120 Z;C09D 11/54;C09D 11/32;C09D...</td>\n","      <td>2C056 EA04;2C056 FB03;2C056 FC01;2C056 HA42;2H...</td>\n","      <td>【請求項１】  染色液によるメディアへの着色度合を調整する単独では前記メディア上で視認困難な...</td>\n","      <td>D06P5/00</td>\n","      <td>2018-06-15</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>特開2019-218522</td>\n","      <td>インクジェットインク</td>\n","      <td>ゼネラル株式会社</td>\n","      <td>C09D11/30;C09D11/38;B41J2/01;B41M5/00;A61K9/20...</td>\n","      <td>C09D 11/30;C09D 11/38;B41J  2/01    501;B41M  ...</td>\n","      <td>2C056 FC01;2H186 BA08;2H186 DA07;2H186 DA12;2H...</td>\n","      <td>【請求項１】  着色剤、アラビアガム、炭素数１２以上、２０以下の高級アルコール、炭素数１以上...</td>\n","      <td>C09D11/30</td>\n","      <td>2018-06-22</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>特開2019-218514</td>\n","      <td>インクジェット記録液セット、インクジェット記録用前処理液の製造方法、印刷物及びインクジェット...</td>\n","      <td>コニカミノルタ株式会社</td>\n","      <td>C09D11/54;C09D11/322;B41J2/01;B41M5/00</td>\n","      <td>C09D 11/54;C09D 11/322;B41J  2/01    123;B41J ...</td>\n","      <td>2C056 EA04;2C056 EA13;2C056 FB02;2C056 FC01;2C...</td>\n","      <td>【請求項１】  少なくとも前処理液とインクとからなるインクジェット記録液セットであって、  ...</td>\n","      <td>C09D11/54</td>\n","      <td>2018-06-22</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>特開2019-218448</td>\n","      <td>水性インクの製造方法</td>\n","      <td>キヤノン株式会社</td>\n","      <td>C09D11/322;C09D11/326;C09D11/38;B41J2/01;B41M5/00</td>\n","      <td>C09D 11/322;C09D 11/326;C09D 11/38;B41J  2/01 ...</td>\n","      <td>2C056 FC01;2H186 FA20;2H186 FB12;2H186 FB15;2H...</td>\n","      <td>【請求項１】  インクジェット用の水性インクの製造方法であって、  顔料、及び界面活性剤を混...</td>\n","      <td>C09D11/32</td>\n","      <td>2018-06-19</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>特開2019-218446</td>\n","      <td>水性インク、インクカートリッジ、記録装置、及び記録方法</td>\n","      <td>富士ゼロックス株式会社</td>\n","      <td>C09D11/322;C09D11/38;B41J2/01;B41M5/00;C09B23/00</td>\n","      <td>C09D 11/322;C09D 11/38;B41J  2/01    501;B41J ...</td>\n","      <td>2C056 FC01;2C056 HA44;2H186 AB12;2H186 BA08;2H...</td>\n","      <td>【請求項１】  水性媒体と、  スクアリリウム骨格を有する化合物と樹脂とを含有する赤外線吸収...</td>\n","      <td>C09D11/32</td>\n","      <td>2018-06-19</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     公報番号(全文リンク)                                             発明等の名称  ... 受容シート 用途\n","0  特開2019-218636        染色物の製造方法、染色調整インクジェットインク、及び染色調整インクジェットインクセット  ...     0  1\n","1  特開2019-218522                                         インクジェットインク  ...     0  0\n","2  特開2019-218514  インクジェット記録液セット、インクジェット記録用前処理液の製造方法、印刷物及びインクジェット...  ...     0  0\n","3  特開2019-218448                                         水性インクの製造方法  ...     0  0\n","4  特開2019-218446                        水性インク、インクカートリッジ、記録装置、及び記録方法  ...     0  0\n","\n","[5 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"37aVJcMQ20LI"},"source":["# 前処理\n","\n","[正規表現](https://docs.python.org/ja/3/howto/regex.html)を使い、特許文書に特有の文字列を削除します。\n","\n","処理した文字列をリスト（processed_data）に収納します。\n","\n","processed_dataをnumpy形式に変換し、保存します。\n","\n","また、先頭から10文書を表示します。"]},{"cell_type":"code","metadata":{"id":"v5s7-Nrl3QB4","executionInfo":{"status":"ok","timestamp":1601718832410,"user_tz":-540,"elapsed":644,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"17225abb-c8a3-4725-8251-3ac6c1875e6b","colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["import re\n","import numpy as np\n","import pandas as pd\n","\n","#  正規表現のコンパイル\n","sep = re.compile('【.*?】|\\n|\\u3000| |\\n') \n","\n","processed_data = []\n","for text in data['独立請求項']:\n","  processed_data.append(sep.sub('', text))\n","processed_data = np.array(processed_data)\n","np.save('data/processed_data.npy', processed_data)\n","processed_data[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['染色液によるメディアへの着色度合を調整する単独では前記メディア上で視認困難な染色調整剤と、色材とを含有する染色調整インクを前記メディアにインクジェット方式により塗布する塗布工程と、前記染色調整インクが塗布された前記メディアを前記染色液に浸漬して染色する染色工程と、を備える、染色物の製造方法。染色液によるメディアへの着色度合を調整する単独では前記メディア上で視認困難な染色調整剤と、前記染色液による染色工程で視認できなくなるまで除去される色材と、を含有する染色調整インクジェットインク。染色液によるメディアへの着色度合を調整する単独では前記メディア上で視認困難な染色調整剤と、前記染色液による染色工程で視認可能な程度に残存する色材と、を含有する染色調整インクジェットインク。染色液によるメディアへの着色度合を調整する単独では前記メディア上で視認困難な染色調整剤と、前記染色液による染色工程で視認可能な程度に残存する第１の色材と、を含有する第１の染色調整インクジェットインクと、前記染色調整剤と、前記染...',\n","       '着色剤、アラビアガム、炭素数１２以上、２０以下の高級アルコール、炭素数１以上、４以下の低級アルコール、および水を含むインクジェットインク。',\n","       '少なくとも前処理液とインクとからなるインクジェット記録液セットであって、前記前処理液が、少なくとも、ポリエステル、ポリオレフィン又はポリウレタンの骨格のいずれかを含む水不溶性樹脂微粒子と凝集剤と水とを含有し、前記インクが、少なくとも、顔料とオキサゾリン基を有する化合物と水とを含有する、ことを特徴とするインクジェット記録液セット。',\n","       'インクジェット用の水性インクの製造方法であって、顔料、及び界面活性剤を混合して混合物を得る工程と、得られた前記混合物、及びその他のインク成分を混合する工程と、を有し、前記顔料が、顔料の粒子表面に、（ｉ）アニオン性基、及び、（ｉｉ）他の原子団とアニオン性基とが結合した基、からなる群より選択される官能基が結合した自己分散顔料であるとともに、前記官能基の密度が、０．１０μｍｏｌ／ｍ2以上０．４８μｍｏｌ／ｍ2以下であり、前記界面活性剤が、陽イオンとなりうる窒素原子が第２級アミン又は第３級アミンの構造を有するアミノ酸型界面活性剤であることを特徴とする水性インクの製造方法。',\n","       '水性媒体と、スクアリリウム骨格を有する化合物と樹脂とを含有する赤外線吸収性粒子と、ジアルキルスルホコハク酸塩と、赤色アゾ顔料と、を含む水性インク。水性媒体と、スクアリリウム骨格を有する化合物と樹脂とを含有する赤外線吸収性粒子と、ジアルキルスルホコハク酸塩と、赤色顔料と、を含み、６０℃の環境下で１４日間経過することによる前記赤外線吸収性粒子における赤外線吸光係数の低下率が５％以下である水性インク。',\n","       '平均粒子径が０．４～２．５μｍであり、屈折率が１．４～１．７であり、比重が２．１以下である粒子を含む、インクジェットインク。',\n","       '発光性ナノ結晶粒子を含有する少なくとも１種の発光性インク組成物と、光散乱性粒子を含有する非発光性インク組成物と、を含む、インク組成物セット。発光性ナノ結晶粒子を含有する少なくとも１種の発光性画素部と、光散乱性粒子を含有する非発光性画素部と、を備える、光変換層。',\n","       '少なくともインクジェットインクと前処理液を含むインクジェット記録液セットであって、前記インクジェットインクが、少なくとも顔料、顔料分散剤、水溶性有機溶媒及び水不溶性樹脂を含有し、当該水不溶性樹脂が、ポリエステル骨格、ポリオレフィン骨格又はポリウレタン骨格のいずれかを含む水不溶性樹脂であり、前記前処理液が、少なくとも凝集剤及び水不溶性樹脂微粒子を含有し、当該水不溶性樹脂微粒子が、ポリオレフィン系樹脂がポリウレタン系樹脂に含有された複合樹脂粒子であることを特徴とするインクジェット記録液セット。',\n","       'インクジェット印刷によって物体に印刷を行う方法であって、以下のステップ：ｉ）物体を用意し、少なくとも１つのインクジェット印刷ヘッドを用いて物体に印刷像を印刷し、その際、前記印刷像は、少なくとも２つのレーン（１，２）から構成されており、各前記レーン（１，２）は、内側に位置する１つのコア領域（３）と外側に位置する２つの縁領域（４）とを有し、少なくとも２つの前記レーン（１，２）は、隣り合う前記レーンの外側に位置する前記縁領域（４）が相互に接し合うまたは部分的に重なり合うように配置されている、ステップと、ｉｉ）ステップｉ）において形成された前記レーン（１，２）にＵＶ放射源（５）を用いてＵＶ放射を照射する、ステップと、を含む、インクジェット印刷によって物体に印刷を行う方法において、ステップｉｉ）の照射時に使用されるＵＶ放射の強度を、ステップｉ）で形成された前記レーン（１，２）の、内側に位置する前記コア領域（３）から外側に位置する前記縁領域（４）の外縁へ向けて低下させることを特徴とする、インクジェット印刷によって物体に印刷を行う方法。',\n","       '記録媒体の画像形成領域にインクを吐出して画像を形成する記録ヘッドと、前記記録ヘッドを保持し、前記記録媒体の搬送方向と交差する主走査方向に往復移動するキャリッジと、特定領域に前記記録ヘッドから前記インクを吐出する画像形成動作を実行させる実行部とを備え、前記実行部は、前記画像形成動作においてパターン画像を形成し、前記特定領域は、前記記録媒体の前記画像形成領域以外の領域を示し、前記パターン画像は、前記記録媒体への前記インクの吐出位置を調整するための画像を示す、インクジェット記録装置。'],\n","      dtype='<U485')"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"09nHsFrHUBLH"},"source":["# 形態素解析\n","\n","文字列を分解し、スペースで区切った新たな文字列を生成します。"]},{"cell_type":"markdown","metadata":{"id":"5ezGBGVPyl7i"},"source":["## MeCab\n","\n","日本語の文法に依存した形態素解析器として最も有名な方法です。\n","\n","MeCabの導入は環境に依存します。\n","\n","[mecab-python3](https://pypi.org/project/mecab-python3/) （[Python3からMeCabを使う](https://qiita.com/taroc/items/b9afd914432da08dafc8)）\n","\n","[natto-py](https://pypi.org/project/natto-py/)（[Python の MeCab バインディング natto-py を使う](https://qiita.com/buruzaemon/items/975027cea6371b2c5ec3)）\n","\n","等を検索して導入してください。"]},{"cell_type":"code","metadata":{"id":"eEQVWkAfxz1d","executionInfo":{"status":"ok","timestamp":1601718912090,"user_tz":-540,"elapsed":71747,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"dcc1a5b6-1128-4219-dd1a-26de12f4645d","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# MeCabのインストール\n","!apt install aptitude swig\n","!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n","!pip install mecab-python3\n","!pip install unidic-lite"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n","  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n","  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n","  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n","  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n","  swig3.0\n","Suggested packages:\n","  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n","  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n","  libwww-perl xapian-tools swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n","  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n","  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n","  libhttp-message-perl libio-html-perl libio-string-perl\n","  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n","  libsub-name-perl libtimedate-perl liburi-perl libxapian30 swig swig3.0\n","0 upgraded, 23 newly installed, 0 to remove and 22 not upgraded.\n","Need to get 4,978 kB of archives.\n","After this operation, 21.4 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 4,978 kB in 1s (3,897 kB/s)\n","Selecting previously unselected package aptitude-common.\n","(Reading database ... 144618 files and directories currently installed.)\n","Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n","Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n","Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n","Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Selecting previously unselected package libcwidget3v5:amd64.\n","Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n","Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n","Selecting previously unselected package libxapian30:amd64.\n","Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n","Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Selecting previously unselected package aptitude.\n","Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n","Unpacking aptitude (0.8.10-6ubuntu1) ...\n","Selecting previously unselected package libhtml-tagset-perl.\n","Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n","Unpacking libhtml-tagset-perl (3.20-3) ...\n","Selecting previously unselected package liburi-perl.\n","Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n","Unpacking liburi-perl (1.73-1) ...\n","Selecting previously unselected package libhtml-parser-perl.\n","Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n","Unpacking libhtml-parser-perl (3.72-3build1) ...\n","Selecting previously unselected package libcgi-pm-perl.\n","Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n","Unpacking libcgi-pm-perl (4.38-1) ...\n","Selecting previously unselected package libfcgi-perl.\n","Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n","Unpacking libfcgi-perl (0.78-2build1) ...\n","Selecting previously unselected package libcgi-fast-perl.\n","Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n","Unpacking libcgi-fast-perl (1:2.13-1) ...\n","Selecting previously unselected package libsub-name-perl.\n","Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n","Unpacking libsub-name-perl (0.21-1build1) ...\n","Selecting previously unselected package libclass-accessor-perl.\n","Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n","Unpacking libclass-accessor-perl (0.51-1) ...\n","Selecting previously unselected package libencode-locale-perl.\n","Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n","Unpacking libencode-locale-perl (1.05-1) ...\n","Selecting previously unselected package libtimedate-perl.\n","Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n","Unpacking libtimedate-perl (2.3000-2) ...\n","Selecting previously unselected package libhttp-date-perl.\n","Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n","Unpacking libhttp-date-perl (6.02-1) ...\n","Selecting previously unselected package libio-html-perl.\n","Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n","Unpacking libio-html-perl (1.001-1) ...\n","Selecting previously unselected package liblwp-mediatypes-perl.\n","Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n","Unpacking liblwp-mediatypes-perl (6.02-1) ...\n","Selecting previously unselected package libhttp-message-perl.\n","Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n","Unpacking libhttp-message-perl (6.14-1) ...\n","Selecting previously unselected package libio-string-perl.\n","Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n","Unpacking libio-string-perl (1.08-3) ...\n","Selecting previously unselected package libparse-debianchangelog-perl.\n","Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n","Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n","Selecting previously unselected package swig3.0.\n","Preparing to unpack .../21-swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../22-swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up libhtml-tagset-perl (3.20-3) ...\n","Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up libencode-locale-perl (1.05-1) ...\n","Setting up libtimedate-perl (2.3000-2) ...\n","Setting up libio-html-perl (1.001-1) ...\n","Setting up aptitude-common (0.8.10-6ubuntu1) ...\n","Setting up liblwp-mediatypes-perl (6.02-1) ...\n","Setting up liburi-perl (1.73-1) ...\n","Setting up libhtml-parser-perl (3.72-3build1) ...\n","Setting up libcgi-pm-perl (4.38-1) ...\n","Setting up libio-string-perl (1.08-3) ...\n","Setting up libsub-name-perl (0.21-1build1) ...\n","Setting up libfcgi-perl (0.78-2build1) ...\n","Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n","Setting up libclass-accessor-perl (0.51-1) ...\n","Setting up swig (3.0.12-1) ...\n","Setting up libhttp-date-perl (6.02-1) ...\n","Setting up libcgi-fast-perl (1:2.13-1) ...\n","Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n","Setting up libhttp-message-perl (6.14-1) ...\n","Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n","Setting up aptitude (0.8.10-6ubuntu1) ...\n","update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.10)\n","xz-utils is already installed at the requested version (5.2.2-1.3)\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.10)\n","xz-utils is already installed at the requested version (5.2.2-1.3)\n","The following NEW packages will be installed:\n","  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n","0 packages upgraded, 11 newly installed, 0 to remove and 22 not upgraded.\n","Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n","Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n","Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n","Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n","Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n","Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n","Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n","Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n","Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n","Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n","Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n","Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n","Fetched 29.3 MB in 2s (16.4 MB/s)\n","Selecting previously unselected package libmagic-mgc.\n","(Reading database ... 145868 files and directories currently installed.)\n","Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package libmagic1:amd64.\n","Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package file.\n","Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n","Unpacking file (1:5.32-2ubuntu0.4) ...\n","Selecting previously unselected package libmecab2:amd64.\n","Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n","Unpacking libmecab2:amd64 (0.996-5) ...\n","Selecting previously unselected package libmecab-dev.\n","Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n","Unpacking libmecab-dev (0.996-5) ...\n","Selecting previously unselected package mecab-utils.\n","Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n","Unpacking mecab-utils (0.996-5) ...\n","Selecting previously unselected package mecab-jumandic-utf8.\n","Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-jumandic.\n","Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n","Unpacking mecab-jumandic (7.0-20130310-4) ...\n","Selecting previously unselected package mecab-ipadic.\n","Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n","Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n","Selecting previously unselected package mecab.\n","Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n","Unpacking mecab (0.996-5) ...\n","Selecting previously unselected package mecab-ipadic-utf8.\n","Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n","Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n","Setting up libmecab2:amd64 (0.996-5) ...\n","Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n","Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n","Setting up mecab-utils (0.996-5) ...\n","Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up libmecab-dev (0.996-5) ...\n","Setting up file (1:5.32-2ubuntu0.4) ...\n","Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n","Compiling Juman dictionary for Mecab.\n","reading /usr/share/mecab/dic/juman/unk.def ... 37\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n","reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n","reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n","reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n","reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n","reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n","reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n","reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n","reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n","reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n","reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n","reading /usr/share/mecab/dic/juman/Special.csv ... 158\n","reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n","reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n","reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n","reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n","Setting up mecab (0.996-5) ...\n","Compiling IPA dictionary for Mecab.  This takes long time...\n","reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n","emitting double-array: 100% |###########################################| \n","/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n","reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n","reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n","reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n","reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n","reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n","reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n","reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n","reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n","reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n","reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n","reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n","reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n","reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n","reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n","reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n","reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n","reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n","reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n","reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n","reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n","reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n","reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n","reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n","reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n","reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n","emitting double-array: 100% |###########################################| \n","reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n","emitting matrix      : 100% |###########################################| \n","\n","done!\n","Setting up mecab-jumandic (7.0-20130310-4) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","                            \n","Collecting mecab-python3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/06/2aeff86243c88580ccf78b136d403ce5e0a1eed9091103157f01e806499f/mecab_python3-1.0.1-cp36-cp36m-manylinux2010_x86_64.whl (3.5MB)\n","\u001b[K     |████████████████████████████████| 3.5MB 2.8MB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.1\n","Collecting unidic-lite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/d2/a4233f65f718f27065a4cf23a2c4f05d8bd4c75821e092060c4efaf28e66/unidic-lite-1.0.7.tar.gz (47.3MB)\n","\u001b[K     |████████████████████████████████| 47.3MB 81kB/s \n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.7-cp36-none-any.whl size=47556594 sha256=004c6965ba7e1bfc1529feb97d291dc3ca66ce9833d0c185697660188fc37d4e\n","  Stored in directory: /root/.cache/pip/wheels/a8/82/7d/086724645e33a575aafd0b1dae2835c37d2c00c6a0a96ee3a0\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WfFZOvln881g"},"source":["numpy形式のテキストデータを読み込み、\n","\n","MeCabで分かち書きしたものをリスト（result）に収納します。"]},{"cell_type":"code","metadata":{"id":"hKda11rz74pT","executionInfo":{"status":"ok","timestamp":1601718917551,"user_tz":-540,"elapsed":776,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"235fa028-b2f0-4831-eea3-95fe8a46b0c0","colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["import MeCab\n","import numpy as pd\n","\n","m = MeCab.Tagger('-Owakati')\n","\n","processed_data = np.load('data/processed_data.npy')\n","result = []\n","for text in processed_data:\n","  result.append(m.parse(text))\n","result = np.array(result)\n","np.save('data/mecab_data.npy', result)\n","result[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['染色 液 に よる メディア へ の 着色 度合 を 調整 する 単独 で は 前記 メディア 上 で 視認 困難 な 染色 調整 剤 と 、 色材 と を 含有 する 染色 調整 インク を 前記 メディア に インク ジェット 方式 に より 塗布 する 塗布 工程 と 、 前記 染色 調整 インク が 塗布 さ れ た 前記 メディア を 前記 染色 液 に 浸漬 し て 染色 する 染色 工程 と 、 を 備える 、 染色 物 の 製造 方法 。 染色 液 に よる メディア へ の 着色 度合 を 調整 する 単独 で は 前記 メディア 上 で 視認 困難 な 染色 調整 剤 と 、 前記 染色 液 に よる 染色 工程 で 視認 でき なく なる まで 除去 さ れる 色材 と 、 を 含有 する 染色 調整 インク ジェット インク 。 染色 液 に よる メディア へ の 着色 度合 を 調整 する 単独 で は 前記 メディア 上 で 視認 困難 な 染色 調整 剤 と 、 前記 染色 液 に よる 染色 工程 で 視認 可能 な 程度 に 残存 する 色材 と 、 を 含有 する 染色 調整 インク ジェット インク 。 染色 液 に よる メディア へ の 着色 度合 を 調整 する 単独 で は 前記 メディア 上 で 視認 困難 な 染色 調整 剤 と 、 前記 染色 液 に よる 染色 工程 で 視認 可能 な 程度 に 残存 する 第 １ の 色材 と 、 を 含有 する 第 １ の 染色 調整 インク ジェット インク と 、 前記 染色 調整 剤 と 、 前記 染 . . . \\n',\n","       '着色 剤 、 アラビア ガム 、 炭素 数 １２ 以上 、 ２０ 以下 の 高級 アルコール 、 炭素 数 １ 以上 、 ４ 以下 の 低級 アルコール 、 お よび水 を 含む インク ジェット インク 。 \\n',\n","       '少なく とも 前 処理 液 と インク と から なる インク ジェット 記録 液 セット で あっ て 、 前記 前 処理 液 が 、 少なく とも 、 ポリエステル 、 ポリオレフィン 又 は ポリウレタン の 骨格 の いずれ か を 含む 水 不溶 性 樹脂 微 粒子 と 凝集 剤 と 水 と を 含有 し 、 前記 インク が 、 少なく とも 、 顔料 と オキサゾリン 基 を 有する 化合 物 と 水 と を 含有 する 、 こと を 特徴 と する インク ジェット 記録 液 セット 。 \\n',\n","       'インク ジェット 用 の 水性 インク の 製造 方法 で あっ て 、 顔料 、 及び 界面 活性 剤 を 混合 し て 混合 物 を 得る 工程 と 、 得 られ た 前記 混合 物 、 及び その 他 の インク 成分 を 混合 する 工程 と 、 を 有し 、 前記 顔料 が 、 顔料 の 粒子 表面 に 、 （ ｉ ） アニオン 性 基 、 及び 、 （ ｉｉ ） 他 の 原子 団 と アニオン 性 基 と が 結合 し た 基 、 から なる 群 より 選択 さ れる 官能 基 が 結合 し た 自己 分散 顔料 で ある と とも に 、 前記 官 能基 の 密度 が 、 ０ ． １０ μｍ ｏｌ ／ｍ 2 以上 ０ ． ４８ μｍ ｏｌ ／ｍ 2 以下 で あり 、 前記 界面 活性 剤 が 、 陽 イオン と なり うる 窒素 原子 が 第 ２ 級 アミン 又 は 第 ３ 級 アミン の 構造 を 有する アミノ 酸 型 界面 活性 剤 で ある こと を 特徴 と する 水性 インク の 製造 方法 。 \\n',\n","       '水性 媒体 と 、 スクアリリウム 骨格 を 有する 化合 物 と 樹脂 と を 含有 する 赤外 線 吸収 性 粒子 と 、 ジアルキルスルホコハク 酸 塩 と 、 赤色 アゾ 顔料 と 、 を 含む 水性 インク 。 水性 媒体 と 、 スクアリリウム 骨格 を 有する 化合 物 と 樹脂 と を 含有 する 赤外 線 吸収 性 粒子 と 、 ジアルキルスルホコハク 酸 塩 と 、 赤色 顔料 と 、 を 含み 、 ６０ ℃ の 環境 下 で １４ 日間 経過 する こと に よる 前記 赤外 線 吸収 性 粒子 に おけ る 赤外 線 吸光 係数 の 低下 率 が ５ ％ 以下 で ある 水性 インク 。 \\n',\n","       '平均 粒子 径 が ０ ． ４ ～ ２ ． ５ μｍ で あり 、 屈折 率 が １ ． ４ ～ １ ． ７ で あり 、 比重 が ２ ． １ 以下 で ある 粒子 を 含む 、 インク ジェット インク 。 \\n',\n","       '発光 性 ナノ 結晶 粒子 を 含有 する 少なく とも １ 種 の 発光 性 インク 組成 物 と 、 光 散乱 性 粒子 を 含有 する 非 発光 性 インク 組成 物 と 、 を 含む 、 インク 組成 物 セット 。 発光 性 ナノ 結晶 粒子 を 含有 する 少なく とも １ 種 の 発光 性 画素 部 と 、 光 散乱 性 粒子 を 含有 する 非 発光 性 画素 部 と 、 を 備える 、 光 変換 層 。 \\n',\n","       '少なく とも インク ジェット インク と 前 処理 液 を 含む インク ジェット 記録 液 セット で あっ て 、 前記 インク ジェット インク が 、 少なく とも 顔料 、 顔料 分散 剤 、 水溶 性 有機 溶媒 及び 水 不溶 性 樹脂 を 含有 し 、 当該 水 不溶 性 樹脂 が 、 ポリエステル 骨格 、 ポリオレフィン 骨格 又 は ポリウレタン 骨格 の いずれ か を 含む 水 不溶 性 樹脂 で あり 、 前記 前 処理 液 が 、 少なく とも 凝集 剤 及び 水 不溶 性 樹脂 微 粒子 を 含有 し 、 当該 水 不溶 性 樹脂 微 粒子 が 、 ポリオレフィン 系 樹脂 が ポリウレタン 系 樹脂 に 含有 さ れ た 複合 樹脂 粒子 で ある こと を 特徴 と する インク ジェット 記録 液 セット 。 \\n',\n","       'インク ジェット 印刷 に よっ て 物体 に 印刷 を 行う 方法 で あっ て 、 以下 の ステップ ： ｉ ） 物体 を 用意 し 、 少なく とも １ つ の インク ジェット 印刷 ヘッド を 用い て 物体 に 印刷 像 を 印刷 し 、 その 際 、 前記 印刷 像 は 、 少なく とも ２ つ の レーン （ １，２ ） から 構成 さ れ て おり 、 各 前記 レーン （ １，２ ） は 、 内側 に 位置 する １ つ の コア 領域 （ ３ ） と 外側 に 位置 する ２ つ の 縁 領域 （ ４ ） と を 有し 、 少なく とも ２ つ の 前記 レーン （ １，２ ） は 、 隣り合う 前記 レーン の 外側 に 位置 する 前記 縁 領域 （ ４ ） が 相互 に 接し 合う また は 部分 的 に 重なり合う よう に 配置 さ れ て いる 、 ステップ と 、 ｉｉ ） ステップ ｉ ） に おい て 形成 さ れ た 前記 レーン （ １，２ ） に ＵＶ 放射 源 （ ５ ） を 用い て ＵＶ 放射 を 照射 する 、 ステップ と 、 を 含む 、 インク ジェット 印刷 に よっ て 物体 に 印刷 を 行う 方法 に おい て 、 ステップ ｉｉ ） の 照射 時 に 使用 さ れる ＵＶ 放射 の 強度 を 、 ステップ ｉ ） で 形成 さ れ た 前記 レーン （ １，２ ） の 、 内側 に 位置 する 前記 コア 領域 （ ３ ） から 外側 に 位置 する 前記 縁 領域 （ ４ ） の 外縁 へ 向け て 低下 さ せる こと を 特徴 と する 、 インク ジェット 印刷 に よっ て 物体 に 印刷 を 行う 方法 。 \\n',\n","       '記録 媒体 の 画像 形成 領域 に インク を 吐出し て 画像 を 形成 する 記録 ヘッド と 、 前記 記録 ヘッド を 保持 し 、 前記 記録 媒体 の 搬送 方向 と 交差 する 主 走査 方向 に 往復 移動 する キャリッジ と 、 特定 領域 に 前記 記録 ヘッド から 前記 インク を 吐出 する 画像 形成 動作 を 実行 さ せる 実行 部 と を 備え 、 前記 実行 部 は 、 前記 画像 形成 動作 に おい て パターン 画像 を 形成 し 、 前記 特定 領域 は 、 前記 記録 媒体 の 前記 画像 形成 領域 以外 の 領域 を 示し 、 前記 パターン 画像 は 、 前記 記録 媒体 へ の 前記 インク の 吐出 位置 を 調整 する ため の 画像 を 示す 、 インク ジェット 記録 装置 。 \\n'],\n","      dtype='<U794')"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"L1Fv87VEA6z_"},"source":["## [sentencepiece](https://pypi.org/project/sentencepiece/)\n","\n","サブワード法として最も有名な方法です。\n","\n","あらかじめ学習済みモデルとして、\n","\n","[BERT with SentencePiece を日本語 Wikipedia で学習してモデルを公開しました｜原理的には可能](https://yoheikikuta.github.io/bert-japanese/)\n","\n","が公開している [google Drive](https://drive.google.com/drive/folders/1Zsm9DD40lrUVu6iAnIuTH2ODIkh-WM-O) 上のデータ から 'wiki-ja.model' をダウンロードして　'./data'　に保存しておきます。\n"]},{"cell_type":"code","metadata":{"id":"2dpbGklbEU0f","executionInfo":{"status":"ok","timestamp":1601718929369,"user_tz":-540,"elapsed":3779,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"ad93e0f2-d7bd-4f29-921b-bbf38144565d","colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 2.6MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.91\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gy4RR6OdBPuv","executionInfo":{"status":"ok","timestamp":1601718933597,"user_tz":-540,"elapsed":1326,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"9518f983-b9d8-4231-954b-39bfb4c66d97","colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["import sentencepiece as spm\n","import numpy as pd\n","\n","sp = spm.SentencePieceProcessor()\n","sp.load('data/wiki-ja.model')\n","\n","processed_data = np.load('data/processed_data.npy')\n","result = []\n","for text in processed_data:\n","  result.append(' '.join(sp.EncodeAsPieces(text)))\n","result = np.array(result)\n","np.save('data/sentencepiece_data.npy', result)\n","result[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['▁ 染色 液 による メディア への 着 色 度 合 を調整する 単独で は 前 記 メディア 上で 視認 困難な 染色 調整 剤 と 、 色 材 と を 含有 する 染色 調整 インク を 前 記 メディア に インク ジェット 方式 により 塗 布 する 塗 布 工程 と 、 前 記 染色 調整 インク が 塗 布 された 前 記 メディア を 前 記 染色 液 に 浸 漬 して 染色 する 染色 工程 と 、 を備える 、 染色 物 の製造 方法 。 染色 液 による メディア への 着 色 度 合 を調整する 単独で は 前 記 メディア 上で 視認 困難な 染色 調整 剤 と 、 前 記 染色 液 による 染色 工程 で 視認 できなくなる まで 除去 される 色 材 と 、 を 含有 する 染色 調整 インク ジェット インク 。 染色 液 による メディア への 着 色 度 合 を調整する 単独で は 前 記 メディア 上で 視認 困難な 染色 調整 剤 と 、 前 記 染色 液 による 染色 工程 で 視認 可能な 程度 に 残存 する 色 材 と 、 を 含有 する 染色 調整 インク ジェット インク 。 染色 液 による メディア への 着 色 度 合 を調整する 単独で は 前 記 メディア 上で 視認 困難な 染色 調整 剤 と 、 前 記 染色 液 による 染色 工程 で 視認 可能な 程度 に 残存 する 第 1 の色 材 と 、 を 含有 する 第 1 の 染色 調整 インク ジェット インク と 、 前 記 染色 調整 剤 と 、 前 記 染 ...',\n","       '▁ 着 色 剤 、 アラビア ガム 、 炭素 数 12 以上 、 20 以下の 高級 アルコール 、 炭素 数 1 以上 、 4 以下の 低 級 アルコール 、 および 水 を含む インク ジェット インク 。',\n","       '▁ 少なくとも 前 処理 液 と インク と からなる インク ジェット 記録 液 セット であって 、 前 記 前 処理 液 が 、 少なくとも 、 ポリ エステル 、 ポリ オレ フィン 又は ポリ ウ レ タン の 骨格 のいずれか を含む 水 不 溶 性 樹脂 微 粒子 と 凝 集 剤 と 水 と を 含有 し 、 前 記 インク が 、 少なくとも 、 顔料 と オキ サ ゾ リン 基 を有する 化合物 と 水 と を 含有 する 、 こと を特徴とする インク ジェット 記録 液 セット 。',\n","       '▁ インク ジェット 用の 水 性 インク の製造 方法 であって 、 顔料 、 及び 界 面 活性 剤 を 混合 して 混合 物 を得る 工程 と 、 得られた 前 記 混合 物 、 及び その他の インク 成分 を 混合 する 工程 と 、 を有し 、 前 記 顔料 が 、 顔料 の 粒子 表面に 、 ( i ) ア ニオン 性 基 、 及び 、 ( ii ) 他の 原子 団 と ア ニオン 性 基 と が 結合 した 基 、 からなる 群 より 選択 される 官 能 基 が 結合 した 自己 分散 顔料 である とともに 、 前 記 官 能 基の 密度 が 、 0.1 0 μ mol / m 2 以上 0.4 8 μ mol / m 2 以下 であり 、 前 記 界 面 活性 剤 が 、 陽 イオン となり うる 窒素 原子 が 第 2 級 アミン 又は 第 3 級 アミン の構造 を有する アミノ酸 型 界 面 活性 剤 であること を特徴とする 水 性 インク の製造 方法 。',\n","       '▁ 水 性 媒体 と 、 スク アリ リウム 骨格 を有する 化合物 と 樹脂 と を 含有 する 赤外線 吸収 性 粒子 と 、 ジ アル キル スル ホ コ ハク 酸 塩 と 、 赤色 ア ゾ 顔料 と 、 を含む 水 性 インク 。 水 性 媒体 と 、 スク アリ リウム 骨格 を有する 化合物 と 樹脂 と を 含有 する 赤外線 吸収 性 粒子 と 、 ジ アル キル スル ホ コ ハク 酸 塩 と 、 赤色 顔料 と 、 を含み 、 6 0° C の 環境 下で 14 日間 経過 すること による 前 記 赤外線 吸収 性 粒子 における 赤外線 吸 光 係数 の低下 率が 5% 以下 である 水 性 インク 。',\n","       '▁ 平均 粒子 径 が 0.4 ～ 2.5 μ m であり 、 屈折 率が 1.4 ～ 1.7 であり 、 比重 が 2.1 以下 である 粒子 を含む 、 インク ジェット インク 。',\n","       '▁ 発光 性 ナノ 結晶 粒子 を 含有 する 少なくとも 1 種の 発光 性 インク 組成 物 と 、 光 散乱 性 粒子 を 含有 する 非 発光 性 インク 組成 物 と 、 を含む 、 インク 組成 物 セット 。 発光 性 ナノ 結晶 粒子 を 含有 する 少なくとも 1 種の 発光 性 画素 部 と 、 光 散乱 性 粒子 を 含有 する 非 発光 性 画素 部 と 、 を備える 、 光 変換 層 。',\n","       '▁ 少なくとも インク ジェット インク と 前 処理 液 を含む インク ジェット 記録 液 セット であって 、 前 記 インク ジェット インク が 、 少なくとも 顔料 、 顔料 分散 剤 、 水 溶 性 有機 溶媒 及び 水 不 溶 性 樹脂 を 含有 し 、 当該 水 不 溶 性 樹脂 が 、 ポリ エステル 骨格 、 ポリ オレ フィン 骨格 又は ポリ ウ レ タン 骨格 のいずれか を含む 水 不 溶 性 樹脂 であり 、 前 記 前 処理 液 が 、 少なくとも 凝 集 剤 及び 水 不 溶 性 樹脂 微 粒子 を 含有 し 、 当該 水 不 溶 性 樹脂 微 粒子 が 、 ポリ オレ フィン 系 樹脂 が ポリ ウ レ タン 系 樹脂 に 含有 された 複合 樹脂 粒子 であること を特徴とする インク ジェット 記録 液 セット 。',\n","       '▁ インク ジェット 印刷 によって 物体 に 印刷 を行う 方法 であって 、 以下の ステップ : i ) 物体 を用意し 、 少なくとも 1 つの インク ジェット 印刷 ヘッド を用いて 物体 に 印刷 像 を 印刷 し 、 その際 、 前 記 印刷 像 は 、 少なくとも 2 つの レーン ( 1,2 ) から構成され ており 、 各 前 記 レーン ( 1,2 ) は 、 内側 に位置する 1 つの コア 領域 ( 3 ) と 外側 に位置する 2 つの 縁 領域 ( 4 ) と を有し 、 少なくとも 2 つの 前 記 レーン ( 1,2 ) は 、 隣 り 合う 前 記 レーン の 外側 に位置する 前 記 縁 領域 ( 4 ) が 相互 に接し 合う または 部分的に 重なり 合う ように 配置 されている 、 ステップ と 、 ii ) ステップ i ) において 形成された 前 記 レーン ( 1,2 ) に UV 放射 源 ( 5 ) を用いて UV 放射 を 照射 する 、 ステップ と 、 を含む 、 インク ジェット 印刷 によって 物体 に 印刷 を行う 方法 において 、 ステップ ii ) の 照射 時 に使用される UV 放射 の 強度 を 、 ステップ i ) で 形成された 前 記 レーン ( 1,2 ) の 、 内側 に位置する 前 記 コア 領域 ( 3 ) から 外側 に位置する 前 記 縁 領域 ( 4 ) の外 縁 へ向けて 低下 させること を特徴とする 、 インク ジェット 印刷 によって 物体 に 印刷 を行う 方法 。',\n","       '▁ 記録 媒体 の 画像 形成 領域 に インク を 吐 出して 画像 を形成する 記録 ヘッド と 、 前 記 記録 ヘッド を保持し 、 前 記 記録 媒体 の 搬送 方向 と 交差する 主 走 査 方向に 往復 移動する キャ リッジ と 、 特定 領域 に 前 記 記録 ヘッド から 前 記 インク を 吐 出 する 画像 形成 動作 を 実行 させる 実行 部 と を備え 、 前 記 実行 部は 、 前 記 画像 形成 動作 において パターン 画像 を形成し 、 前 記 特定 領域 は 、 前 記 記録 媒体 の前 記 画像 形成 領域 以外の 領域 を示し 、 前 記 パターン 画像 は 、 前 記 記録 媒体 への 前 記 インク の 吐 出 位置 を調整する ための 画像 を示す 、 インク ジェット 記録 装置 。'],\n","      dtype='<U777')"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"uakHv4wbBwYe","executionInfo":{"status":"ok","timestamp":1601718936124,"user_tz":-540,"elapsed":642,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"915d9d89-8b7f-437f-e143-1c1e34ea9dff","colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["# おまけ　テキストを単語ID列に変換することもできます。\n","import sentencepiece as spm\n","import numpy as pd\n","\n","sp = spm.SentencePieceProcessor()\n","sp.load('data/wiki-ja.model')\n","result = []\n","maxlen = 0\n","for text in processed_data:\n","  processed_text = sp.EncodeAsIds(text)\n","  result.append(processed_text)\n","  if len(processed_text) > maxlen:\n","    maxlen = len(processed_text)\n","# zero padding で文書の長さをを揃える\n","for i, processed_text in enumerate(result):\n","  result[i] = result[i]+[0]*(maxlen-len(result[i]))\n","result = np.array(result)\n","print(result[:10], '\\n', result.shape, '次元')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[    9 25062  3591 ...     0     0     0]\n"," [    9   447   502 ...     0     0     0]\n"," [    9  4694   217 ...     0     0     0]\n"," ...\n"," [    9  4694 17230 ...     0     0     0]\n"," [    9 17230  4965 ...     0     0     0]\n"," [    9   659 12324 ...     0     0     0]] \n"," (50, 304) 次元\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FK6Eozu1OxGo"},"source":["## n-gram\n","\n","単純に n文字 で分割しますが、nが2以上の時、重なり合うように切り出す工夫が必要です。"]},{"cell_type":"code","metadata":{"id":"SH3T3ieYMb3D","executionInfo":{"status":"ok","timestamp":1601718944008,"user_tz":-540,"elapsed":792,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"6f5f091e-72d9-41a9-b633-fcfb79c04872","colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["n = 3 \n","\n","def ngram(words, n):\n","  # n文字ずつ切り出し、1文字×nのタプル生成\n","  # ('染', '色', '液'), ('色', '液', 'に'), ('液', 'に', 'よ')\n","  ngram = list(zip(*(words[i:] for i in range(n))))\n","  # n文字の文字列リストに変換　'染色液', '色液に', '液によ', 'による'\n","  # リストの要素をスペースで連結して一つの文字列にする\n","  return ' '.join([''.join(j) for j in ngram])\n","\n","processed_data = np.load('data/processed_data.npy')\n","result = []\n","for text in processed_data:\n","  result.append(ngram(text, n))\n","result = np.array(result)\n","np.save('data/'+str(n)+'gram_data.npy', result)\n","result[:10]\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['染色液 色液に 液によ による よるメ るメデ メディ ディア ィアへ アへの への着 の着色 着色度 色度合 度合を 合を調 を調整 調整す 整する する単 る単独 単独で 独では では前 は前記 前記メ 記メデ メディ ディア ィア上 ア上で 上で視 で視認 視認困 認困難 困難な 難な染 な染色 染色調 色調整 調整剤 整剤と 剤と、 と、色 、色材 色材と 材とを とを含 を含有 含有す 有する する染 る染色 染色調 色調整 調整イ 整イン インク ンクを クを前 を前記 前記メ 記メデ メディ ディア ィアに アにイ にイン インク ンクジ クジェ ジェッ ェット ット方 ト方式 方式に 式によ により より塗 り塗布 塗布す 布する する塗 る塗布 塗布工 布工程 工程と 程と、 と、前 、前記 前記染 記染色 染色調 色調整 調整イ 整イン インク ンクが クが塗 が塗布 塗布さ 布され された れた前 た前記 前記メ 記メデ メディ ディア ィアを アを前 を前記 前記染 記染色 染色液 色液に 液に浸 に浸漬 浸漬し 漬して して染 て染色 染色す 色する する染 る染色 染色工 色工程 工程と 程と、 と、を 、を備 を備え 備える える、 る、染 、染色 染色物 色物の 物の製 の製造 製造方 造方法 方法。 法。染 。染色 染色液 色液に 液によ による よるメ るメデ メディ ディア ィアへ アへの への着 の着色 着色度 色度合 度合を 合を調 を調整 調整す 整する する単 る単独 単独で 独では では前 は前記 前記メ 記メデ メディ ディア ィア上 ア上で 上で視 で視認 視認困 認困難 困難な 難な染 な染色 染色調 色調整 調整剤 整剤と 剤と、 と、前 、前記 前記染 記染色 染色液 色液に 液によ による よる染 る染色 染色工 色工程 工程で 程で視 で視認 視認で 認でき できな きなく なくな くなる なるま るまで まで除 で除去 除去さ 去され される れる色 る色材 色材と 材と、 と、を 、を含 を含有 含有す 有する する染 る染色 染色調 色調整 調整イ 整イン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンク。 ク。染 。染色 染色液 色液に 液によ による よるメ るメデ メディ ディア ィアへ アへの への着 の着色 着色度 色度合 度合を 合を調 を調整 調整す 整する する単 る単独 単独で 独では では前 は前記 前記メ 記メデ メディ ディア ィア上 ア上で 上で視 で視認 視認困 認困難 困難な 難な染 な染色 染色調 色調整 調整剤 整剤と 剤と、 と、前 、前記 前記染 記染色 染色液 色液に 液によ による よる染 る染色 染色工 色工程 工程で 程で視 で視認 視認可 認可能 可能な 能な程 な程度 程度に 度に残 に残存 残存す 存する する色 る色材 色材と 材と、 と、を 、を含 を含有 含有す 有する する染 る染色 染色調 色調整 調整イ 整イン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンク。 ク。染 。染色 染色液 色液に 液によ による よるメ るメデ メディ ディア ィアへ アへの への着 の着色 着色度 色度合 度合を 合を調 を調整 調整す 整する する単 る単独 単独で 独では では前 は前記 前記メ 記メデ メディ ディア ィア上 ア上で 上で視 で視認 視認困 認困難 困難な 難な染 な染色 染色調 色調整 調整剤 整剤と 剤と、 と、前 、前記 前記染 記染色 染色液 色液に 液によ による よる染 る染色 染色工 色工程 工程で 程で視 で視認 視認可 認可能 可能な 能な程 な程度 程度に 度に残 に残存 残存す 存する する第 る第１ 第１の １の色 の色材 色材と 材と、 と、を 、を含 を含有 含有す 有する する第 る第１ 第１の １の染 の染色 染色調 色調整 調整イ 整イン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンクと クと、 と、前 、前記 前記染 記染色 染色調 色調整 調整剤 整剤と 剤と、 と、前 、前記 前記染 記染. 染.. ...',\n","       '着色剤 色剤、 剤、ア 、アラ アラビ ラビア ビアガ アガム ガム、 ム、炭 、炭素 炭素数 素数１ 数１２ １２以 ２以上 以上、 上、２ 、２０ ２０以 ０以下 以下の 下の高 の高級 高級ア 級アル アルコ ルコー コール ール、 ル、炭 、炭素 炭素数 素数１ 数１以 １以上 以上、 上、４ 、４以 ４以下 以下の 下の低 の低級 低級ア 級アル アルコ ルコー コール ール、 ル、お 、およ および よび水 び水を 水を含 を含む 含むイ むイン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンク。',\n","       '少なく なくと くとも とも前 も前処 前処理 処理液 理液と 液とイ とイン インク ンクと クとか とから からな らなる なるイ るイン インク ンクジ クジェ ジェッ ェット ット記 ト記録 記録液 録液セ 液セッ セット ットで トであ であっ あって って、 て、前 、前記 前記前 記前処 前処理 処理液 理液が 液が、 が、少 、少な 少なく なくと くとも とも、 も、ポ 、ポリ ポリエ リエス エステ ステル テル、 ル、ポ 、ポリ ポリオ リオレ オレフ レフィ フィン ィン又 ン又は 又はポ はポリ ポリウ リウレ ウレタ レタン タンの ンの骨 の骨格 骨格の 格のい のいず いずれ ずれか れかを かを含 を含む 含む水 む水不 水不溶 不溶性 溶性樹 性樹脂 樹脂微 脂微粒 微粒子 粒子と 子と凝 と凝集 凝集剤 集剤と 剤と水 と水と 水とを とを含 を含有 含有し 有し、 し、前 、前記 前記イ 記イン インク ンクが クが、 が、少 、少な 少なく なくと くとも とも、 も、顔 、顔料 顔料と 料とオ とオキ オキサ キサゾ サゾリ ゾリン リン基 ン基を 基を有 を有す 有する する化 る化合 化合物 合物と 物と水 と水と 水とを とを含 を含有 含有す 有する する、 る、こ 、こと ことを とを特 を特徴 特徴と 徴とす とする するイ るイン インク ンクジ クジェ ジェッ ェット ット記 ト記録 記録液 録液セ 液セッ セット ット。',\n","       'インク ンクジ クジェ ジェッ ェット ット用 ト用の 用の水 の水性 水性イ 性イン インク ンクの クの製 の製造 製造方 造方法 方法で 法であ であっ あって って、 て、顔 、顔料 顔料、 料、及 、及び 及び界 び界面 界面活 面活性 活性剤 性剤を 剤を混 を混合 混合し 合して して混 て混合 混合物 合物を 物を得 を得る 得る工 る工程 工程と 程と、 と、得 、得ら 得られ られた れた前 た前記 前記混 記混合 混合物 合物、 物、及 、及び 及びそ びその その他 の他の 他のイ のイン インク ンク成 ク成分 成分を 分を混 を混合 混合す 合する する工 る工程 工程と 程と、 と、を 、を有 を有し 有し、 し、前 、前記 前記顔 記顔料 顔料が 料が、 が、顔 、顔料 顔料の 料の粒 の粒子 粒子表 子表面 表面に 面に、 に、（ 、（ｉ （ｉ） ｉ）ア ）アニ アニオ ニオン オン性 ン性基 性基、 基、及 、及び 及び、 び、（ 、（ｉ （ｉｉ ｉｉ） ｉ）他 ）他の 他の原 の原子 原子団 子団と 団とア とアニ アニオ ニオン オン性 ン性基 性基と 基とが とが結 が結合 結合し 合した した基 た基、 基、か 、から からな らなる なる群 る群よ 群より より選 り選択 選択さ 択され される れる官 る官能 官能基 能基が 基が結 が結合 結合し 合した した自 た自己 自己分 己分散 分散顔 散顔料 顔料で 料であ である あると るとと ととも ともに もに、 に、前 、前記 前記官 記官能 官能基 能基の 基の密 の密度 密度が 度が、 が、０ 、０． ０．１ ．１０ １０μ ０μｍ μｍｏ ｍｏｌ ｏｌ／ ｌ／ｍ ／ｍ2 ｍ2以 2以上 以上０ 上０． ０．４ ．４８ ４８μ ８μｍ μｍｏ ｍｏｌ ｏｌ／ ｌ／ｍ ／ｍ2 ｍ2以 2以下 以下で 下であ であり あり、 り、前 、前記 前記界 記界面 界面活 面活性 活性剤 性剤が 剤が、 が、陽 、陽イ 陽イオ イオン オンと ンとな となり なりう りうる うる窒 る窒素 窒素原 素原子 原子が 子が第 が第２ 第２級 ２級ア 級アミ アミン ミン又 ン又は 又は第 は第３ 第３級 ３級ア 級アミ アミン ミンの ンの構 の構造 構造を 造を有 を有す 有する するア るアミ アミノ ミノ酸 ノ酸型 酸型界 型界面 界面活 面活性 活性剤 性剤で 剤であ である あるこ ること ことを とを特 を特徴 特徴と 徴とす とする する水 る水性 水性イ 性イン インク ンクの クの製 の製造 製造方 造方法 方法。',\n","       '水性媒 性媒体 媒体と 体と、 と、ス 、スク スクア クアリ アリリ リリウ リウム ウム骨 ム骨格 骨格を 格を有 を有す 有する する化 る化合 化合物 合物と 物と樹 と樹脂 樹脂と 脂とを とを含 を含有 含有す 有する する赤 る赤外 赤外線 外線吸 線吸収 吸収性 収性粒 性粒子 粒子と 子と、 と、ジ 、ジア ジアル アルキ ルキル キルス ルスル スルホ ルホコ ホコハ コハク ハク酸 ク酸塩 酸塩と 塩と、 と、赤 、赤色 赤色ア 色アゾ アゾ顔 ゾ顔料 顔料と 料と、 と、を 、を含 を含む 含む水 む水性 水性イ 性イン インク ンク。 ク。水 。水性 水性媒 性媒体 媒体と 体と、 と、ス 、スク スクア クアリ アリリ リリウ リウム ウム骨 ム骨格 骨格を 格を有 を有す 有する する化 る化合 化合物 合物と 物と樹 と樹脂 樹脂と 脂とを とを含 を含有 含有す 有する する赤 る赤外 赤外線 外線吸 線吸収 吸収性 収性粒 性粒子 粒子と 子と、 と、ジ 、ジア ジアル アルキ ルキル キルス ルスル スルホ ルホコ ホコハ コハク ハク酸 ク酸塩 酸塩と 塩と、 と、赤 、赤色 赤色顔 色顔料 顔料と 料と、 と、を 、を含 を含み 含み、 み、６ 、６０ ６０℃ ０℃の ℃の環 の環境 環境下 境下で 下で１ で１４ １４日 ４日間 日間経 間経過 経過す 過する するこ ること ことに とによ による よる前 る前記 前記赤 記赤外 赤外線 外線吸 線吸収 吸収性 収性粒 性粒子 粒子に 子にお におけ おける ける赤 る赤外 赤外線 外線吸 線吸光 吸光係 光係数 係数の 数の低 の低下 低下率 下率が 率が５ が５％ ５％以 ％以下 以下で 下であ である ある水 る水性 水性イ 性イン インク ンク。',\n","       '平均粒 均粒子 粒子径 子径が 径が０ が０． ０．４ ．４～ ４～２ ～２． ２．５ ．５μ ５μｍ μｍで ｍであ であり あり、 り、屈 、屈折 屈折率 折率が 率が１ が１． １．４ ．４～ ４～１ ～１． １．７ ．７で ７であ であり あり、 り、比 、比重 比重が 重が２ が２． ２．１ ．１以 １以下 以下で 下であ である ある粒 る粒子 粒子を 子を含 を含む 含む、 む、イ 、イン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンク。',\n","       '発光性 光性ナ 性ナノ ナノ結 ノ結晶 結晶粒 晶粒子 粒子を 子を含 を含有 含有す 有する する少 る少な 少なく なくと くとも とも１ も１種 １種の 種の発 の発光 発光性 光性イ 性イン インク ンク組 ク組成 組成物 成物と 物と、 と、光 、光散 光散乱 散乱性 乱性粒 性粒子 粒子を 子を含 を含有 含有す 有する する非 る非発 非発光 発光性 光性イ 性イン インク ンク組 ク組成 組成物 成物と 物と、 と、を 、を含 を含む 含む、 む、イ 、イン インク ンク組 ク組成 組成物 成物セ 物セッ セット ット。 ト。発 。発光 発光性 光性ナ 性ナノ ナノ結 ノ結晶 結晶粒 晶粒子 粒子を 子を含 を含有 含有す 有する する少 る少な 少なく なくと くとも とも１ も１種 １種の 種の発 の発光 発光性 光性画 性画素 画素部 素部と 部と、 と、光 、光散 光散乱 散乱性 乱性粒 性粒子 粒子を 子を含 を含有 含有す 有する する非 る非発 非発光 発光性 光性画 性画素 画素部 素部と 部と、 と、を 、を備 を備え 備える える、 る、光 、光変 光変換 変換層 換層。',\n","       '少なく なくと くとも ともイ もイン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンクと クと前 と前処 前処理 処理液 理液を 液を含 を含む 含むイ むイン インク ンクジ クジェ ジェッ ェット ット記 ト記録 記録液 録液セ 液セッ セット ットで トであ であっ あって って、 て、前 、前記 前記イ 記イン インク ンクジ クジェ ジェッ ェット ットイ トイン インク ンクが クが、 が、少 、少な 少なく なくと くとも とも顔 も顔料 顔料、 料、顔 、顔料 顔料分 料分散 分散剤 散剤、 剤、水 、水溶 水溶性 溶性有 性有機 有機溶 機溶媒 溶媒及 媒及び 及び水 び水不 水不溶 不溶性 溶性樹 性樹脂 樹脂を 脂を含 を含有 含有し 有し、 し、当 、当該 当該水 該水不 水不溶 不溶性 溶性樹 性樹脂 樹脂が 脂が、 が、ポ 、ポリ ポリエ リエス エステ ステル テル骨 ル骨格 骨格、 格、ポ 、ポリ ポリオ リオレ オレフ レフィ フィン ィン骨 ン骨格 骨格又 格又は 又はポ はポリ ポリウ リウレ ウレタ レタン タン骨 ン骨格 骨格の 格のい のいず いずれ ずれか れかを かを含 を含む 含む水 む水不 水不溶 不溶性 溶性樹 性樹脂 樹脂で 脂であ であり あり、 り、前 、前記 前記前 記前処 前処理 処理液 理液が 液が、 が、少 、少な 少なく なくと くとも とも凝 も凝集 凝集剤 集剤及 剤及び 及び水 び水不 水不溶 不溶性 溶性樹 性樹脂 樹脂微 脂微粒 微粒子 粒子を 子を含 を含有 含有し 有し、 し、当 、当該 当該水 該水不 水不溶 不溶性 溶性樹 性樹脂 樹脂微 脂微粒 微粒子 粒子が 子が、 が、ポ 、ポリ ポリオ リオレ オレフ レフィ フィン ィン系 ン系樹 系樹脂 樹脂が 脂がポ がポリ ポリウ リウレ ウレタ レタン タン系 ン系樹 系樹脂 樹脂に 脂に含 に含有 含有さ 有され された れた複 た複合 複合樹 合樹脂 樹脂粒 脂粒子 粒子で 子であ である あるこ ること ことを とを特 を特徴 特徴と 徴とす とする するイ るイン インク ンクジ クジェ ジェッ ェット ット記 ト記録 記録液 録液セ 液セッ セット ット。',\n","       'インク ンクジ クジェ ジェッ ェット ット印 ト印刷 印刷に 刷によ によっ よって って物 て物体 物体に 体に印 に印刷 印刷を 刷を行 を行う 行う方 う方法 方法で 法であ であっ あって って、 て、以 、以下 以下の 下のス のステ ステッ テップ ップ： プ：ｉ ：ｉ） ｉ）物 ）物体 物体を 体を用 を用意 用意し 意し、 し、少 、少な 少なく なくと くとも とも１ も１つ １つの つのイ のイン インク ンクジ クジェ ジェッ ェット ット印 ト印刷 印刷ヘ 刷ヘッ ヘッド ッドを ドを用 を用い 用いて いて物 て物体 物体に 体に印 に印刷 印刷像 刷像を 像を印 を印刷 印刷し 刷し、 し、そ 、その その際 の際、 際、前 、前記 前記印 記印刷 印刷像 刷像は 像は、 は、少 、少な 少なく なくと くとも とも２ も２つ ２つの つのレ のレー レーン ーン（ ン（１ （１， １，２ ，２） ２）か ）から から構 ら構成 構成さ 成され されて れてお ており おり、 り、各 、各前 各前記 前記レ 記レー レーン ーン（ ン（１ （１， １，２ ，２） ２）は ）は、 は、内 、内側 内側に 側に位 に位置 位置す 置する する１ る１つ １つの つのコ のコア コア領 ア領域 領域（ 域（３ （３） ３）と ）と外 と外側 外側に 側に位 に位置 位置す 置する する２ る２つ ２つの つの縁 の縁領 縁領域 領域（ 域（４ （４） ４）と ）とを とを有 を有し 有し、 し、少 、少な 少なく なくと くとも とも２ も２つ ２つの つの前 の前記 前記レ 記レー レーン ーン（ ン（１ （１， １，２ ，２） ２）は ）は、 は、隣 、隣り 隣り合 り合う 合う前 う前記 前記レ 記レー レーン ーンの ンの外 の外側 外側に 側に位 に位置 位置す 置する する前 る前記 前記縁 記縁領 縁領域 領域（ 域（４ （４） ４）が ）が相 が相互 相互に 互に接 に接し 接し合 し合う 合うま うまた または たは部 は部分 部分的 分的に 的に重 に重な 重なり なり合 り合う 合うよ うよう ように うに配 に配置 配置さ 置され されて れてい ている いる、 る、ス 、ステ ステッ テップ ップと プと、 と、ｉ 、ｉｉ ｉｉ） ｉ）ス ）ステ ステッ テップ ップｉ プｉ） ｉ）に ）にお におい おいて いて形 て形成 形成さ 成され された れた前 た前記 前記レ 記レー レーン ーン（ ン（１ （１， １，２ ，２） ２）に ）にＵ にＵＶ ＵＶ放 Ｖ放射 放射源 射源（ 源（５ （５） ５）を ）を用 を用い 用いて いてＵ てＵＶ ＵＶ放 Ｖ放射 放射を 射を照 を照射 照射す 射する する、 る、ス 、ステ ステッ テップ ップと プと、 と、を 、を含 を含む 含む、 む、イ 、イン インク ンクジ クジェ ジェッ ェット ット印 ト印刷 印刷に 刷によ によっ よって って物 て物体 物体に 体に印 に印刷 印刷を 刷を行 を行う 行う方 う方法 方法に 法にお におい おいて いて、 て、ス 、ステ ステッ テップ ップｉ プｉｉ ｉｉ） ｉ）の ）の照 の照射 照射時 射時に 時に使 に使用 使用さ 用され される れるＵ るＵＶ ＵＶ放 Ｖ放射 放射の 射の強 の強度 強度を 度を、 を、ス 、ステ ステッ テップ ップｉ プｉ） ｉ）で ）で形 で形成 形成さ 成され された れた前 た前記 前記レ 記レー レーン ーン（ ン（１ （１， １，２ ，２） ２）の ）の、 の、内 、内側 内側に 側に位 に位置 位置す 置する する前 る前記 前記コ 記コア コア領 ア領域 領域（ 域（３ （３） ３）か ）から から外 ら外側 外側に 側に位 に位置 位置す 置する する前 る前記 前記縁 記縁領 縁領域 領域（ 域（４ （４） ４）の ）の外 の外縁 外縁へ 縁へ向 へ向け 向けて けて低 て低下 低下さ 下させ させる せるこ ること ことを とを特 を特徴 特徴と 徴とす とする する、 る、イ 、イン インク ンクジ クジェ ジェッ ェット ット印 ト印刷 印刷に 刷によ によっ よって って物 て物体 物体に 体に印 に印刷 印刷を 刷を行 を行う 行う方 う方法 方法。',\n","       '記録媒 録媒体 媒体の 体の画 の画像 画像形 像形成 形成領 成領域 領域に 域にイ にイン インク ンクを クを吐 を吐出 吐出し 出して して画 て画像 画像を 像を形 を形成 形成す 成する する記 る記録 記録ヘ 録ヘッ ヘッド ッドと ドと、 と、前 、前記 前記記 記記録 記録ヘ 録ヘッ ヘッド ッドを ドを保 を保持 保持し 持し、 し、前 、前記 前記記 記記録 記録媒 録媒体 媒体の 体の搬 の搬送 搬送方 送方向 方向と 向と交 と交差 交差す 差する する主 る主走 主走査 走査方 査方向 方向に 向に往 に往復 往復移 復移動 移動す 動する するキ るキャ キャリ ャリッ リッジ ッジと ジと、 と、特 、特定 特定領 定領域 領域に 域に前 に前記 前記記 記記録 記録ヘ 録ヘッ ヘッド ッドか ドから から前 ら前記 前記イ 記イン インク ンクを クを吐 を吐出 吐出す 出する する画 る画像 画像形 像形成 形成動 成動作 動作を 作を実 を実行 実行さ 行させ させる せる実 る実行 実行部 行部と 部とを とを備 を備え 備え、 え、前 、前記 前記実 記実行 実行部 行部は 部は、 は、前 、前記 前記画 記画像 画像形 像形成 形成動 成動作 動作に 作にお におい おいて いてパ てパタ パター ターン ーン画 ン画像 画像を 像を形 を形成 形成し 成し、 し、前 、前記 前記特 記特定 特定領 定領域 領域は 域は、 は、前 、前記 前記記 記記録 記録媒 録媒体 媒体の 体の前 の前記 前記画 記画像 画像形 像形成 形成領 成領域 領域以 域以外 以外の 外の領 の領域 領域を 域を示 を示し 示し、 し、前 、前記 前記パ 記パタ パター ターン ーン画 ン画像 画像は 像は、 は、前 、前記 前記記 記記録 記録媒 録媒体 媒体へ 体への への前 の前記 前記イ 記イン インク ンクの クの吐 の吐出 吐出位 出位置 位置を 置を調 を調整 調整す 整する するた るため ための めの画 の画像 画像を 像を示 を示す 示す、 す、イ 、イン インク ンクジ クジェ ジェッ ェット ット記 ト記録 記録装 録装置 装置。'],\n","      dtype='<U1931')"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"1dxGxmAZUFcN"},"source":["# ベクトル化\n","\n","形態素に分割されたテキストを数値化します。\n","\n","参考：[機械学習 〜 テキスト特徴量（CountVectorizer, TfidfVectorizer） 〜](https://qiita.com/fujin/items/b1a7152c2ec2b4963160)"]},{"cell_type":"markdown","metadata":{"id":"09Xl8ciUX4gI"},"source":["## Bag of Words\n","\n","scikit-leran モジュールの [Countvectrizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) を使用して、単語の出現頻度を求めます。\n","\n","参考：[【python】sklearnのCountVectorizerの使い方｜静かなる名辞](https://www.haya-programming.com/entry/2018/02/25/044525)　ほか多数\n","\n","注意：\n","\n","１）すべての文書を読み込んで一気にベクトル化する必要があります。\n","\n","２）データ量が多いので [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) 形式で出力されます。"]},{"cell_type":"code","metadata":{"id":"1lk4xfdcQobv","executionInfo":{"status":"ok","timestamp":1601718953970,"user_tz":-540,"elapsed":1876,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"9536e159-40db-4e24-d992-939c6a47c428","colab":{"base_uri":"https://localhost:8080/","height":617}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","from scipy.io import mmwrite, mmread\n","\n","# MeCab、sentencepiece、3-gram で分割したファイルを読み込みます。\n","files = [['MeCab', 'data/mecab_data.npy'], \n","         ['sentencepiece', 'data/sentencepiece_data.npy'], \n","         ['3-gram', 'data/3gram_data.npy']\n","         ]\n","vectorizer = CountVectorizer(ngram_range=(1, 1), analyzer='word')\n","\n","for method, file in files:\n","  print(method+'で分割した文書をベクトル化')\n","  corpus = np.load(file, allow_pickle=True)\n","  count_vec = vectorizer.fit_transform(corpus) # csr_matrix形式\n","  savefile = file.replace('_data.npy', '_BoW_csr')\n","  mmwrite(savefile, csr_matrix(count_vec))\n","  # 再読込、表示\n","  loaddata = mmread(savefile+'.mtx').todense()\n","  print(loaddata, '\\n', loaddata.shape, '次元', '最大値', np.max(loaddata))\n","  # 形態素(0-5番目、100-105番目)表示\n","  print(vectorizer.get_feature_names()[:5], \n","        vectorizer.get_feature_names()[100:105], '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MeCabで分割した文書をベクトル化\n","[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]] \n"," (50, 627) 次元 最大値 25\n","['11', '12', 'μｍ', 'あっ', 'あり'] ['ジェット', 'ジエチル', 'ジメチルアミノプロピルアクリルアミド', 'スクアリリウム', 'スチレン'] \n","\n","sentencepieceで分割した文書をベクトル化\n","[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 1 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]] \n"," (50, 737) 次元 最大値 24\n","['00', '10', '100', '11', '12'] ['それぞれ', 'ただし', 'ため', 'ために', 'ための'] \n","\n","3-gramで分割した文書をベクトル化\n","[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]] \n"," (50, 4907) 次元 最大値 16\n","['0はｈ', '11の', '11は', '12', '12は'] ['かす走', 'かつ', 'かつｒ', 'から', 'からな'] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FyrCW8QySJ0C"},"source":["## TF-IDF\n","scikit-leran モジュールの [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) を使用して、単語のレア度を求めます。\n","\n","他の注意事項はCountVectrizer と同様です。"]},{"cell_type":"code","metadata":{"id":"5N9okRLQdOkP","executionInfo":{"status":"ok","timestamp":1601718981568,"user_tz":-540,"elapsed":1455,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"4828f065-7b0f-4d92-ede7-04d4d9cf4508","colab":{"base_uri":"https://localhost:8080/","height":617}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","from scipy.io import mmwrite, mmread\n","\n","# MeCab、sentencepiece、3-gram で分割したファイルを読み込みます。\n","files = [['MeCab', 'data/mecab_data.npy'], \n","         ['sentencepiece', 'data/sentencepiece_data.npy'], \n","         ['3-gram', 'data/3gram_data.npy']\n","         ]\n","vectorizer = TfidfVectorizer(ngram_range=(1, 1), analyzer='word')\n","\n","for method, file in files:\n","  print(method+'で分割した文書をベクトル化')\n","  corpus = np.load(file, allow_pickle=True)\n","  tfidf_vec = vectorizer.fit_transform(corpus) # csr_matrix形式\n","  savefile = file.replace('_data.npy', '_TfIdf_csr')\n","  mmwrite(savefile, csr_matrix(tfidf_vec))\n","  # 再読込、表示\n","  loaddata = mmread(savefile+'.mtx').todense()\n","  print(loaddata, '\\n', loaddata.shape, '次元', '最大値', np.max(loaddata))\n","  # 形態素(0-5番目、100-105番目)表示\n","  print(vectorizer.get_feature_names()[:5], \n","        vectorizer.get_feature_names()[100:105], '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MeCabで分割した文書をベクトル化\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] \n"," (50, 627) 次元 最大値 0.7843879333604795\n","['11', '12', 'μｍ', 'あっ', 'あり'] ['ジェット', 'ジエチル', 'ジメチルアミノプロピルアクリルアミド', 'スクアリリウム', 'スチレン'] \n","\n","sentencepieceで分割した文書をベクトル化\n","[[0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.24694266 0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," ...\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]] \n"," (50, 737) 次元 最大値 0.7466183366424842\n","['00', '10', '100', '11', '12'] ['それぞれ', 'ただし', 'ため', 'ために', 'ための'] \n","\n","3-gramで分割した文書をベクトル化\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] \n"," (50, 4907) 次元 最大値 0.4217329265792593\n","['0はｈ', '11の', '11は', '12', '12は'] ['かす走', 'かつ', 'かつｒ', 'から', 'からな'] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HtigX_MFTWZM"},"source":["CountVectrizer も TfidfVectorizer も入力文の形態素を自動抽出して計算しますが、\n","\n","vocabulary に単語のリストを与えると、与えた単語に対して計算します。"]},{"cell_type":"code","metadata":{"id":"3iy7y6lwdl5R","executionInfo":{"status":"ok","timestamp":1601718993746,"user_tz":-540,"elapsed":530,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"2b111beb-a484-477c-84b1-7bf951812488","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","from scipy.io import mmwrite, mmread\n","\n","# MeCab、sentencepiece、3-gram で分割したファイルを読み込みます。\n","files = [['MeCab', 'data/mecab_data.npy'], \n","         ['sentencepiece', 'data/sentencepiece_data.npy'], \n","         ['3-gram', 'data/3gram_data.npy']\n","         ]\n","vocab = ['備える', 'ノニオン', 'ペロブスカイト', 'ポリエステル', \n","         'ポリオレフィン', 'ポリカーボネート', '印刷', '原子', '重なり']\n","\n","for method, file in files:\n","  print(method+'で分割した文書をベクトル化', '\\nBoW')\n","  corpus = np.load(file, allow_pickle=True)\n","  vectorizer = CountVectorizer(ngram_range=(1, 1), analyzer='word', vocabulary=vocab)\n","  print(vectorizer.get_feature_names())\n","  count_vec = vectorizer.fit_transform(corpus) # csr_matrix形式\n","  print(count_vec.toarray()[:10], '\\n', count_vec.shape, '次元', '最大値', np.max(count_vec.toarray()), '\\nTfIdf')\n","  vectorizer = TfidfVectorizer(ngram_range=(1, 1), analyzer='word', vocabulary=vocab)\n","  tfidf_vec = vectorizer.fit_transform(corpus) # csr_matrix形式\n","  print(tfidf_vec.toarray()[:10], '\\n', tfidf_vec.shape, '次元', '最大値', np.max(tfidf_vec.toarray()), '\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MeCabで分割した文書をベクトル化 \n","BoW\n","['備える', 'ノニオン', 'ペロブスカイト', 'ポリエステル', 'ポリオレフィン', 'ポリカーボネート', '印刷', '原子', '重なり']\n","[[ 1  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  1  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  2  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 1  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  1  2  0  0  0  0]\n"," [ 0  0  0  0  0  0 10  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]] \n"," (50, 9) 次元 最大値 11 \n","TfIdf\n","[[1.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.67902226 0.73411768 0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         1.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [1.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.41975887 0.90763566 0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  1.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]] \n"," (50, 9) 次元 最大値 1.0 \n","\n","sentencepieceで分割した文書をベクトル化 \n","BoW\n","['備える', 'ノニオン', 'ペロブスカイト', 'ポリエステル', 'ポリオレフィン', 'ポリカーボネート', '印刷', '原子', '重なり']\n","[[ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  2  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0 10  0  1]\n"," [ 0  0  0  0  0  0  0  0  0]] \n"," (50, 9) 次元 最大値 11 \n","TfIdf\n","[[0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         1.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.98913839 0.         0.1469872 ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.        ]] \n"," (50, 9) 次元 最大値 1.0 \n","\n","3-gramで分割した文書をベクトル化 \n","BoW\n","['備える', 'ノニオン', 'ペロブスカイト', 'ポリエステル', 'ポリオレフィン', 'ポリカーボネート', '印刷', '原子', '重なり']\n","[[1 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0]\n"," [1 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1]\n"," [0 0 0 0 0 0 0 0 0]] \n"," (50, 9) 次元 最大値 3 \n","TfIdf\n","[[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n"," (50, 9) 次元 最大値 1.0 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EcFlsEiNYnRM"},"source":["## HashingVectrizer\n","\n","[feature hashing](https://ja.wikipedia.org/wiki/Feature_Hashing)という手法を使った埋め込み表現です。\n","\n","scikit-leran モジュールの [HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) を使用して計算します。\n","\n","単語情報は設定した(n_features)次元に分散されます。ベクトルの要素と単語とは1対1対応ではありません。"]},{"cell_type":"code","metadata":{"id":"nEcfwY73UJbG","executionInfo":{"status":"ok","timestamp":1602188372313,"user_tz":-540,"elapsed":9977,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"35388321-d1df-4179-b36b-ab4d41fd0227","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.feature_extraction.text import HashingVectorizer\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","from scipy.io import mmwrite, mmread\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","\n","# MeCab、sentencepiece、3-gram で分割したファイルを読み込みます。\n","files = [['MeCab', 'data/mecab_data.npy'], \n","         ['sentencepiece', 'data/sentencepiece_data.npy'], \n","         ['3-gram', 'data/3gram_data.npy']\n","         ]\n","n_features = [1048576, 1024]\n","\n","for method, file in files:\n","  print(method+'で分割した文書をベクトル化')\n","  corpus = np.load(file, allow_pickle=True)[:100]\n","  for i in range(len(n_features)):\n","    vectorizer = HashingVectorizer(ngram_range=(1, 1), analyzer='word', n_features=n_features[i])\n","    hash_vec = vectorizer.fit_transform(corpus) # csr_matrix形式\n","    savefile = file.replace('_data.npy', '_Hash'+str(n_features[i])+'_csr')\n","    mmwrite(savefile, csr_matrix(hash_vec))\n","    # 再読込、表示\n","    loaddata = mmread(savefile+'.mtx').todense()\n","    print(loaddata, '\\n', loaddata.shape, '次元', '最大値', np.max(loaddata))\n","  print()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["MeCabで分割した文書をベクトル化\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] \n"," (100, 1048576) 次元 最大値 0.7671992600910912\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] \n"," (100, 1024) 次元 最大値 0.7671992600910912\n","\n","sentencepieceで分割した文書をベクトル化\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] \n"," (50, 1048576) 次元 最大値 0.6986670116874985\n","[[ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," ...\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.         -0.04956816  0.         ...  0.          0.\n","   0.        ]] \n"," (50, 1024) 次元 最大値 0.6986670116874985\n","\n","3-gramで分割した文書をベクトル化\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]] \n"," (50, 1048576) 次元 最大値 0.3693584275765589\n","[[ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.06608186  0.         ...  0.         -0.06608186\n","   0.        ]\n"," ...\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [-0.1048645   0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]] \n"," (50, 1024) 次元 最大値 0.3601412030328099\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7fBNaAVyb2iD"},"source":["## Doc2Vec\n","\n","gensim モジュールの [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) を用いて、\n","\n","文章中の隣り合う単語の出現頻度を予測する学習を行い、\n","\n","得られたモデルに未知の文書を入力すると、学習時に設定した次元のベクトルが得られます。\n","\n","学習に用いたタグを入力すると、学習に用いた文書のベクトルが得られます。\n","\n","(参考)\n","\n","[Doc2Vecについてまとめる](https://qiita.com/g-k/items/5ea94c13281f675302ca)\n","\n","[Doc2Vecの仕組みとgensimを使った文書類似度算出チュートリアル](https://deepage.net/machine_learning/2017/01/08/doc2vec.html)\n","\n","[Word2Vecとは | 分散表現・Skip-gram法とCBOWの仕組み・ツールや活用事例まで徹底解説](https://ledge.ai/word2vec/)"]},{"cell_type":"code","metadata":{"id":"CXVQ4NivUMAT","executionInfo":{"status":"ok","timestamp":1601719021430,"user_tz":-540,"elapsed":4558,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"282bdcb7-2ac0-4d10-c3ab-d7385890a47c","colab":{"base_uri":"https://localhost:8080/","height":890}},"source":["import numpy as np\n","import pandas as pd\n","from gensim.models.doc2vec import Doc2Vec\n","from gensim.models.doc2vec import TaggedDocument\n","from scipy.sparse import csr_matrix\n","from scipy.io import mmwrite, mmread\n","import joblib\n","\n","files = [['MeCab', 'data/mecab_data.npy'], \n","         ['sentencepiece', 'data/sentencepiece_data.npy'], \n","         ['3-gram', 'data/3gram_data.npy']\n","         ]\n","\n","for method, file in files:\n","  print(method+'で分割した文書をベクトル化')\n","  corpus = np.load(file, allow_pickle=True)\n","  training_docs = []\n","  for i in range(len(corpus)):\n","    training_docs.append(TaggedDocument(words=corpus[i],\n","                       tags=[data['公報番号(全文リンク)'][i]]))\n","  # dm=0 でDoc2Vec 学習\n","  model = Doc2Vec(documents=training_docs, vector_size=300, min_count=1, dm=0)\n","  joblib.dump(model, 'data/doc2vec_dm0_'+method+'.model', compress=3)\n","\n","  # 学習モデルに文書を入力しベクトルを得る\n","  doc_vec = []\n","  for i in range(len(data)):\n","    doc_vec.append(model.infer_vector(training_docs[i].tags))\n","  savefile = 'data/x_doc2vec_dm0_'+method\n","  mmwrite(savefile, csr_matrix(np.array(doc_vec)))\n","  # 再読込、表示\n","  loaddata = mmread(savefile+'.mtx').todense()\n","  print(loaddata, '\\n', loaddata.shape, '次元', '最大値', np.max(loaddata), '\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MeCabで分割した文書をベクトル化\n","[[-0.00119889 -0.00156107  0.00095843 ...  0.00043311  0.00051889\n","  -0.00165555]\n"," [ 0.00083766  0.00089225 -0.00108992 ... -0.00050002 -0.00135353\n","   0.00035542]\n"," [ 0.00138262 -0.00062317  0.00074927 ...  0.00062413 -0.00128545\n","  -0.00150257]\n"," ...\n"," [ 0.0012527   0.00055048 -0.00115774 ... -0.00150503  0.00158021\n","  -0.00068571]\n"," [-0.00044679  0.00072698 -0.00121772 ... -0.0009695  -0.00015943\n","   0.00031917]\n"," [ 0.00143875 -0.00037791 -0.00137032 ...  0.00124565 -0.00107917\n","  -0.00156544]] \n"," (50, 300) 次元 最大値 0.0016665872 \n","\n","sentencepieceで分割した文書をベクトル化\n","[[-0.00119889 -0.00156107  0.00095843 ...  0.00043311  0.00051889\n","  -0.00165555]\n"," [ 0.00083766  0.00089225 -0.00108992 ... -0.00050002 -0.00135353\n","   0.00035542]\n"," [ 0.00138262 -0.00062317  0.00074927 ...  0.00062413 -0.00128545\n","  -0.00150257]\n"," ...\n"," [ 0.0012527   0.00055048 -0.00115774 ... -0.00150503  0.00158021\n","  -0.00068571]\n"," [-0.00044679  0.00072698 -0.00121772 ... -0.0009695  -0.00015943\n","   0.00031917]\n"," [ 0.00143875 -0.00037791 -0.00137032 ...  0.00124565 -0.00107917\n","  -0.00156544]] \n"," (50, 300) 次元 最大値 0.0016665872 \n","\n","3-gramで分割した文書をベクトル化\n","[[-0.00119889 -0.00156107  0.00095843 ...  0.00043311  0.00051889\n","  -0.00165555]\n"," [ 0.00083766  0.00089225 -0.00108992 ... -0.00050002 -0.00135353\n","   0.00035542]\n"," [ 0.00138262 -0.00062317  0.00074927 ...  0.00062413 -0.00128545\n","  -0.00150257]\n"," ...\n"," [ 0.0012527   0.00055048 -0.00115774 ... -0.00150503  0.00158021\n","  -0.00068571]\n"," [-0.00044679  0.00072698 -0.00121772 ... -0.0009695  -0.00015943\n","   0.00031917]\n"," [ 0.00143875 -0.00037791 -0.00137032 ...  0.00124565 -0.00107917\n","  -0.00156544]] \n"," (50, 300) 次元 最大値 0.0016665872 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dFX_5ubQBT-T"},"source":["## ELMo\n","\n","双方向LSTMを用いて学習させた言語モデルで、文脈を考慮した単語埋め込み表現が得られます。[実装](https://pypi.org/project/elmoformanylangs/)\n","\n","（参考）\n","\n","[大規模日本語ビジネスニュースコーパスを学習したELMo（MeCab利用）モデルの紹介](https://qiita.com/mkt3/items/9577b63900109ff91665)\n","\n","[大規模日本語ビジネスニュースコーパスを学習したELMo（MeCab利用）モデルの利用方法と精度比較検証](https://qiita.com/kaeru_nantoka/items/bca53a2daea2b29c9b39)"]},{"cell_type":"code","metadata":{"id":"ZE0sz9Pd5g1C","executionInfo":{"status":"ok","timestamp":1602194343889,"user_tz":-540,"elapsed":4115,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"3e453648-89d0-4066-e411-3559aebcb0ca","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 必要なライブラリをインストールします。\n","%cd '/content/drive/My Drive/'\n","!pip install overrides\n","!git clone https://github.com/HIT-SCIR/ELMoForManyLangs.git\n","!sudo python 'ELMoForManyLangs/setup.py' install\n","\n","\n","# 必要なライブラリをインポートします。\n","import numpy as np\n","import pandas as pd\n","import torch\n","from ELMoForManyLangs.elmoformanylangs import Embedder\n","from overrides import overrides\n","from IPython.display import clear_output"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (3.1.0)\n","fatal: destination path 'ELMoForManyLangs' already exists and is not an empty directory.\n","running install\n","running bdist_egg\n","running egg_info\n","writing elmoformanylangs.egg-info/PKG-INFO\n","writing dependency_links to elmoformanylangs.egg-info/dependency_links.txt\n","writing requirements to elmoformanylangs.egg-info/requires.txt\n","writing top-level names to elmoformanylangs.egg-info/top_level.txt\n","reading manifest file 'elmoformanylangs.egg-info/SOURCES.txt'\n","writing manifest file 'elmoformanylangs.egg-info/SOURCES.txt'\n","installing library code to build/bdist.linux-x86_64/egg\n","running install_lib\n","warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n","\n","creating build/bdist.linux-x86_64/egg\n","creating build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","copying elmoformanylangs.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n","zip_safe flag not set; analyzing archive contents...\n","creating 'dist/elmoformanylangs-0.0.2-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n","removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n","Processing elmoformanylangs-0.0.2-py3.6.egg\n","Removing /usr/local/lib/python3.6/dist-packages/elmoformanylangs-0.0.2-py3.6.egg\n","Copying elmoformanylangs-0.0.2-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n","elmoformanylangs 0.0.2 is already the active version in easy-install.pth\n","\n","Installed /usr/local/lib/python3.6/dist-packages/elmoformanylangs-0.0.2-py3.6.egg\n","Processing dependencies for elmoformanylangs==0.0.2\n","Searching for overrides==3.1.0\n","Best match: overrides 3.1.0\n","Adding overrides 3.1.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for numpy==1.18.5\n","Best match: numpy 1.18.5\n","Adding numpy 1.18.5 to easy-install.pth file\n","Installing f2py script to /usr/local/bin\n","Installing f2py3 script to /usr/local/bin\n","Installing f2py3.6 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for h5py==2.10.0\n","Best match: h5py 2.10.0\n","Adding h5py 2.10.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for torch==1.6.0+cu101\n","Best match: torch 1.6.0+cu101\n","Adding torch 1.6.0+cu101 to easy-install.pth file\n","Installing convert-caffe2-to-onnx script to /usr/local/bin\n","Installing convert-onnx-to-caffe2 script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for six==1.15.0\n","Best match: six 1.15.0\n","Adding six 1.15.0 to easy-install.pth file\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Searching for future==0.16.0\n","Best match: future 0.16.0\n","Adding future 0.16.0 to easy-install.pth file\n","Installing futurize script to /usr/local/bin\n","Installing pasteurize script to /usr/local/bin\n","\n","Using /usr/local/lib/python3.6/dist-packages\n","Finished processing dependencies for elmoformanylangs==0.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I2JgQAB6SLNH"},"source":["（作業）\n","\n","https://drive.google.com/drive/u/1/folders/1sau1I10rFeAn8BDk8eZDL5qaEjTlNghp\n","\n","こちらから　単語単位埋め込みモデル　と　文字単位・単語単位埋め込みモデル　とをダウンロードし、マイドライブにアップロードします。"]},{"cell_type":"code","metadata":{"id":"l_I-bpvRCgSZ","executionInfo":{"status":"ok","timestamp":1602196257831,"user_tz":-540,"elapsed":302120,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"95e59fb3-5a91-4105-8a6e-758cc01da242","colab":{"base_uri":"https://localhost:8080/","height":708}},"source":["# ELMo\n","\n","def serial_mode(txt_batch):\n","  data = []\n","  for k in range(len(txt_batch)):\n","    torch.cuda.empty_cache()\n","    data.append(char_e.sents2elmo([txt_batch[k]], output_layer=output_layer))\n","  return data\n","\n","word_model_path = './単語単位埋め込みモデル'\n","char_model_path = './文字単位・単語単位埋め込みモデル'\n","\n","# 文字単位・単語単位埋め込みモデルを読み込み\n","char_e = Embedder(model_dir=char_model_path, batch_size=64)\n","output_layer=-1 # モデルの最終段の出力を得る\n","\n","corpus = np.load('data/mecab_data.npy', allow_pickle=True)\n","\n","texts = []\n","new_array = []\n","n_batch = 50\n","print('\\nコーパス整理中')\n","for i in range(len(corpus)):\n","  texts.append(corpus[i].split(' '))\n","\n","for i in range(len(corpus)//n_batch):\n","  if i%30 == 0:\n","    clear_output()\n","  print('\\rELMo変換中\\t', \n","        str(n_batch*i)+'-'+str(n_batch*(i+1))+'/'+str(len(corpus)), end='\\t')\n","  try:\n","    data = char_e.sents2elmo(texts[n_batch*i:n_batch*(i+1)], output_layer=output_layer)\n","    # 可変長ベクトルの平均をとり固定長ベクトルにする\n","    for j in range(len(data)):\n","      new_array.append(np.average(data[j], axis=0).reshape(1,-1))\n","    torch.cuda.empty_cache()\n","  except:\n","    # n_batch数の処理でメモリオーバーフローを起こすとき1文書ずつ処理する\n","    print('serial mode')\n","    data = serial_mode(texts[n_batch*i:n_batch*i+1])\n","    for j in range(len(data)):\n","      new_array.append(np.average(data[j][0], axis=0).reshape(1,-1))\n","      torch.cuda.empty_cache()\n","    print('\\rELMo変換中\\t', \n","        str(n_batch*i)+'-'+str(len(corpus))+'/'+str(len(corpus)), end='\\t')\n","data = serial_mode(texts[n_batch*(i+1):])\n","for j in range(len(data)):\n","  new_array.append(np.average(data[j][0], axis=0).reshape(1,-1))\n","\n","new_array = np.vstack(new_array)\n","np.save('data/x_ELMo_mecab.npy', new_array)\n","print('実行結果', new_array.shape, '\\n', new_array)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["\rELMo変換中\t 4500-4550/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:12,219 INFO: 1 batches, avg len: 192.0\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4550-4600/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:15,052 INFO: 1 batches, avg len: 186.6\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4600-4650/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:17,747 INFO: 1 batches, avg len: 180.1\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4650-4700/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:20,426 INFO: 1 batches, avg len: 187.0\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4700-4750/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:23,409 INFO: 1 batches, avg len: 186.1\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4750-4800/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:26,091 INFO: 1 batches, avg len: 186.4\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4800-4850/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:28,919 INFO: 1 batches, avg len: 205.5\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4850-4900/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:31,798 INFO: 1 batches, avg len: 190.6\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4900-4950/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:34,599 INFO: 1 batches, avg len: 201.6\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 4950-5000/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:37,486 INFO: 1 batches, avg len: 173.1\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 5000-5050/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:40,128 INFO: 1 batches, avg len: 161.6\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 5050-5100/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:42,755 INFO: 1 batches, avg len: 179.9\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 5100-5150/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:45,454 INFO: 1 batches, avg len: 189.6\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 5150-5200/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:48,266 INFO: 1 batches, avg len: 216.6\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 5200-5250/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:51,078 INFO: 1 batches, avg len: 185.3\n","2020-10-08 22:30:51,837 INFO: 1 batches, avg len: 69.0\n"],"name":"stderr"},{"output_type":"stream","text":["serial mode\n","ELMo変換中\t 5250-5300/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:52,567 INFO: 1 batches, avg len: 185.1\n","2020-10-08 22:30:53,251 INFO: 1 batches, avg len: 97.0\n"],"name":"stderr"},{"output_type":"stream","text":["serial mode\n","ELMo変換中\t 5300-5350/5351\t"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:54,603 INFO: 1 batches, avg len: 377.2\n","2020-10-08 22:30:54,633 INFO: 1 batches, avg len: 86.0\n"],"name":"stderr"},{"output_type":"stream","text":["serial mode\n"],"name":"stdout"},{"output_type":"stream","text":["2020-10-08 22:30:55,349 INFO: 1 batches, avg len: 467.0\n"],"name":"stderr"},{"output_type":"stream","text":["\rELMo変換中\t 5300-5351/5351\t実行結果 (5204, 1024) \n"," [[-0.33146614  0.20417939  0.20073496 ...  0.6192389   0.22071181\n","  -0.10123414]\n"," [-0.02435132  0.28076112  0.06769402 ...  0.7251096   0.15409651\n","  -0.25710547]\n"," [-0.01607116  0.21535724  0.41298237 ...  0.67664057  0.4805104\n","   0.18667226]\n"," ...\n"," [ 0.06666844  0.4897722   0.19548763 ...  0.7980848   0.2269311\n","  -0.18554683]\n"," [-0.06452872  0.41726747  0.14705794 ...  0.72102714  0.19435036\n","  -0.04705095]\n"," [-0.15871945  0.1290972   0.35129866 ...  0.60404783  0.05334947\n","  -0.43363634]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"crtGdyCmmGVV","executionInfo":{"status":"ok","timestamp":1602195894773,"user_tz":-540,"elapsed":1153,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"a0ec219d-cf54-4497-9d36-5851964eca91","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["np.average(data[j][0], axis=0).shape"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1024,)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"wSaPaVo6KfkV"},"source":["# sentence-transformers日本語版\n","\n","https://github.com/sonoisa/sentence-transformers\n","\n","（参考）\n","\n","[はじめての自然言語処理｜第9回 Sentence BERT による類似文章検索の検証](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part9.html)"]},{"cell_type":"code","metadata":{"id":"jf-tXVQxAH4D","executionInfo":{"status":"ok","timestamp":1602196945235,"user_tz":-540,"elapsed":5592,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"91333ccd-1bef-4041-b237-c75d3e6e851b","colab":{"base_uri":"https://localhost:8080/","height":128}},"source":["! pip install mecab-python3==0.996.5"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting mecab-python3==0.996.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n","\u001b[K     |████████████████████████████████| 17.1MB 229kB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-0.996.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W731JqPZKeuK","executionInfo":{"status":"ok","timestamp":1602196961855,"user_tz":-540,"elapsed":9077,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"54fe32f8-a418-4c62-e582-667054e38692","colab":{"base_uri":"https://localhost:8080/","height":892}},"source":["%cd '/content/drive/My Drive/'\n","!git clone https://github.com/sonoisa/sentence-transformers\n","!cd sentence-transformers; pip install -r requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content\n","fatal: destination path 'sentence-transformers' already exists and is not an empty directory.\n","Collecting transformers==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n","\u001b[K     |████████████████████████████████| 450kB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.41.1)\n","Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.996.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 19.0MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 29.6MB/s \n","\u001b[?25hCollecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/34/bdef270a8ae5cbf2b89a2dfe9a477cd63ae5ea9a5740e6412c6c320cc2da/boto3-1.15.15-py2.py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 40.7MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->-r requirements.txt (line 3)) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 7)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 1)) (7.1.2)\n","Collecting botocore<1.19.0,>=1.18.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/83/db265be17aa6e1a8b24cd9032752a5c46d504a9ed654121793c98bbfbb9a/botocore-1.18.15-py2.py3-none-any.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 33.9MB/s \n","\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 11.7MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.19.0,>=1.18.15->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b3cb24ca4d12bd52d0b7039c6941e9dce0e674b08350d50ba242e2826783ff4d\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, jmespath, botocore, s3transfer, boto3, transformers\n","Successfully installed boto3-1.15.15 botocore-1.18.15 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91 transformers-2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mj334FvTLOg5","executionInfo":{"status":"ok","timestamp":1602196993400,"user_tz":-540,"elapsed":20138,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"1254f105-8a03-4bad-a487-6b3dbaa28db7","colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["!wget -O sonobe-datasets-sentence-transformers-model.tar \"https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2\"\n","!tar -xvf sonobe-datasets-sentence-transformers-model.tar\n","%cd '/content/drive/My Drive/sentence-transformers/'\n","from sentence_transformers import SentenceTransformer\n","%cd '/content/drive/My Drive/'\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2020-10-08 22:42:57--  https://www.floydhub.com/api/v1/resources/JLTtbaaK5dprnxoJtUbBbi?content=true&download=true&rename=sonobe-datasets-sentence-transformers-model-2\n","Resolving www.floydhub.com (www.floydhub.com)... 104.26.0.30, 172.67.72.144, 104.26.1.30, ...\n","Connecting to www.floydhub.com (www.floydhub.com)|104.26.0.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/tar]\n","Saving to: ‘sonobe-datasets-sentence-transformers-model.tar’\n","\n","sonobe-datasets-sen     [                <=> ] 422.28M  40.6MB/s    in 10s     \n","\n","2020-10-08 22:43:07 (42.2 MB/s) - ‘sonobe-datasets-sentence-transformers-model.tar’ saved [442788352]\n","\n","./\n","./training_bert_japanese/\n","./training_bert_japanese/0_BERTJapanese/\n","./training_bert_japanese/0_BERTJapanese/added_tokens.json\n","./training_bert_japanese/0_BERTJapanese/config.json\n","./training_bert_japanese/0_BERTJapanese/pytorch_model.bin\n","./training_bert_japanese/0_BERTJapanese/sentence_bert_config.json\n","./training_bert_japanese/0_BERTJapanese/special_tokens_map.json\n","./training_bert_japanese/0_BERTJapanese/tokenizer_config.json\n","./training_bert_japanese/0_BERTJapanese/vocab.txt\n","./training_bert_japanese/1_Pooling/\n","./training_bert_japanese/1_Pooling/config.json\n","./training_bert_japanese/config.json\n","./training_bert_japanese/modules.json\n","/content/drive/My Drive/sentence-transformers\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KAuRL6VPOZzz","executionInfo":{"status":"ok","timestamp":1602197096485,"user_tz":-540,"elapsed":25165,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}}},"source":["%tensorflow_version 2.x\n","import numpy as np\n","model_path = '/content/drive/My Drive/training_bert_japanese'\n","model = SentenceTransformer(model_path, show_progress_bar=False)\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejBp9vVKh-Z5","executionInfo":{"status":"ok","timestamp":1602197653578,"user_tz":-540,"elapsed":53761,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"45ffa36c-c2bf-486f-92db-b6ec62446d74","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# document vector\n","import joblib\n","import pandas as pd\n","import numpy as np\n","from scipy.sparse import csr_matrix\n","\n","corpus = np.load('data/mecab_data.npy', allow_pickle=True)\n","\n","vectors = model.encode(corpus)\n","vectors = np.vstack(vectors)\n","np.save('data/x_ELMo_mecab.npy', vectors)\n","print('実行結果', vectors.shape, '\\n', vectors)\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["実行結果 (5328, 768) \n"," [[ 0.30623767  0.9199255  -0.3692814  ... -0.45889604 -0.18640393\n","  -1.1709334 ]\n"," [ 0.74890316 -0.36533457 -0.14062168 ... -0.44990322 -0.39594465\n","  -0.5531663 ]\n"," [ 0.57097554 -0.07667922 -0.4922952  ... -0.4587596  -0.92465496\n","  -1.009658  ]\n"," ...\n"," [ 0.04443025  0.57329595 -0.780079   ...  0.75817376 -0.42281306\n","  -1.6895096 ]\n"," [ 0.95789146 -0.29498622 -1.1318222  ...  0.7242595   0.4687451\n","  -1.7129064 ]\n"," [ 0.6381206   0.0237502  -0.5740135  ...  0.4319682  -0.42214027\n","  -0.73637563]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cl1MghTaoS5u"},"source":["## ラベルエンコード、one-hot ベクトル\n","\n","Tensorflow.Kerasモジュールのテキスト処理クラスである[Tokenizer](tf.keras.preprocessing.text.Tokenizer)に文書を与えることによって、\n","\n","単語IDの列からなる文書ベクトルが得られます。\n","\n","このとき、sequenceで各文書の長さをそろえますが、短い文書の後ろに 0 を付ける（zero padding）のが主流です。\n","\n","長い文書に揃えず、適当な長さで区切って、余った単語を切り捨てる方法もあります。\n","\n","参考：[Keras Documentation](https://keras.io/ja/preprocessing/text/)\n","\n","また、[np_utilsクラス](https://keras.io/ja/utils/np_utils/)の to_categorical API に数値nを入力すると、\n","\n","n番目が1で残りが0のベクトル（one-hotベクトル）が得られます。\n","\n","参考：[Keras Documentation](https://keras.io/ja/)"]},{"cell_type":"code","metadata":{"id":"rkxVvDcmlva3","executionInfo":{"status":"ok","timestamp":1601719035969,"user_tz":-540,"elapsed":2278,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"326b31f6-ccee-4fca-c248-ddf11d88070a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# ラベルエンコード\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing import sequence\n","\n","files = [['MeCab', 'data/mecab_data.npy'], \n","         ['sentencepiece', 'data/sentencepiece_data.npy'], \n","         ['3-gram', 'data/3gram_data.npy']\n","]\n","\n","for method, file in files:\n","  print(method+'で分割した文書をベクトル化')\n","  corpus = np.load(file, allow_pickle=True)\n","  # 文字列をID列に変換\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(corpus)\n","  list_tokenized = tokenizer.texts_to_sequences(corpus)\n","  # ID列の長さを揃える\n","  list_sequence = sequence.pad_sequences(list_tokenized, maxlen=None, dtype='int32', \n","                                        padding='post', value=0.0)\n","  list_tokenized = np.array(list_sequence)\n","  print(list_tokenized, '\\n', list_tokenized.shape, '次元', '最大値', np.max(list_tokenized), '\\n')\n","  # ID列の長さを50で打ち切る\n","  list_sequence = sequence.pad_sequences(list_tokenized, maxlen=50, dtype='int32', \n","                                        padding='post', value=0.0)\n","  list_tokenized = np.array(list_sequence)\n","  print(list_tokenized, '\\n', list_tokenized.shape, '次元', '最大値', np.max(list_tokenized), '\\n')  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["MeCabで分割した文書をベクトル化\n","[[ 59  92   5 ...   0   0   0]\n"," [155  55   1 ...   0   0   0]\n"," [ 43  40 202 ...   0   0   0]\n"," ...\n"," [  6 133   3 ...   0   0   0]\n"," [103 132 107 ...   0   0   0]\n"," [342 343  48 ...   0   0   0]] \n"," (50, 307) 次元 最大値 796 \n","\n","[[ 55   7   1 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]\n"," ...\n"," [133   3 168 ...   0   0   0]\n"," [  4 198 559 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]] \n"," (50, 50) 次元 最大値 791 \n","\n","sentencepieceで分割した文書をベクトル化\n","[[ 27  49  42 ...   0   0   0]\n"," [ 27  63  89 ...   0   0   0]\n"," [ 27  37   3 ...   0   0   0]\n"," ...\n"," [ 27   4  42 ...   0   0   0]\n"," [ 27  92 137 ...   0   0   0]\n"," [ 27 383 384 ...   0   0   0]] \n"," (50, 281) 次元 最大値 1000 \n","\n","[[  8  49 133 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]\n"," ...\n"," [ 34  10   4 ...   0   0   0]\n"," [  1 128   9 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]] \n"," (50, 50) 次元 最大値 992 \n","\n","3-gramで分割した文書をベクトル化\n","[[ 256  257  304 ...    0    0    0]\n"," [2719 2720 2721 ...    0    0    0]\n"," [  22   25   26 ...    0    0    0]\n"," ...\n"," [   1  488  489 ...    0    0    0]\n"," [ 247  248  145 ...    0    0    0]\n"," [ 940  941  942 ...    0    0    0]] \n"," (50, 482) 次元 最大値 5513 \n","\n","[[161   1 125 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]\n"," ...\n"," [ 36  37  38 ...   0   0   0]\n"," [ 29 387  50 ...   0   0   0]\n"," [  0   0   0 ...   0   0   0]] \n"," (50, 50) 次元 最大値 5430 \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0dVnDPer1hu","executionInfo":{"status":"ok","timestamp":1601719037668,"user_tz":-540,"elapsed":551,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"caaea7b4-0d65-435f-a3c0-f729f975faa1","colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["# one-hotベクトル(1)\n","# Tensorflowに取り込まれる前のKerasのutilityにあるnp_utilsを用いる方法\n","\n","from keras.utils import np_utils\n","\n","index_data = [0, 1, 0, 2, 0, 1, 0, 2]\n","np_utils.to_categorical(index_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"f3V0S3ljrtLk","executionInfo":{"status":"ok","timestamp":1601719038879,"user_tz":-540,"elapsed":605,"user":{"displayName":"西尾潤","photoUrl":"","userId":"08290973370176560667"}},"outputId":"0180a7a7-943b-4f35-8da6-08dc5c0c3a07","colab":{"base_uri":"https://localhost:8080/","height":217}},"source":["# one-hotベクトル(2)\n","# 単位行列を生成するNumPyのeye関数で単位行列を生成し、数値をかける方法\n","\n","import numpy as np\n","\n","index_data = [0, 1, 0, 2, 0, 1, 0, 2]\n","print(np.eye(max(index_data)+1)) # 3×3単位行列\n","np.eye(max(index_data)+1)[index_data]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1. 0. 0.]\n"," [0. 1. 0.]\n"," [0. 0. 1.]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.],\n","       [1., 0., 0.],\n","       [0., 1., 0.],\n","       [1., 0., 0.],\n","       [0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":18}]}]}